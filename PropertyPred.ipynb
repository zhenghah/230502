{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import tdc\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tdc.single_pred\n",
    "import tdc.benchmark_group\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.device_count()\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "import rdkit.Chem.Fragments\n",
    "\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare\n",
    "* functions for computing precision recall accuracy\n",
    "* read files of Pretrain_SMILES_STR_LIST and SymbolDictionary which are obtained during pretraining\n",
    "* Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Threshold = torch.tensor([0.5]).to(device) \n",
    "\n",
    "def positive(proba):\n",
    "    a = (proba >= Threshold) \n",
    "    return a\n",
    "\n",
    "def negative(proba):\n",
    "    a = (proba < Threshold) \n",
    "    return a\n",
    "\n",
    "def seek_TP(pred_y, y):\n",
    "  tp = (positive(pred_y) & positive(y)).int()\n",
    "  return tp\n",
    "\n",
    "def seek_TN(pred_y, y):\n",
    "  tn = (negative(pred_y) & negative(y)).int()\n",
    "  return tn\n",
    "\n",
    "\n",
    "\n",
    "LIMIT_SMILES_LENGTH = 100\n",
    "\n",
    "import pickle\n",
    "f = open('CHEMBL/OdorCode-40 Pretrain_SMILES_STR_LIST','rb')\n",
    "Pretrain_SMILES_STR_LIST = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('CHEMBL/OdorCode-40 Symbol Dictionary', 'rb')\n",
    "[symbol_ID, ID_symbol, sID] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "PAD_ID = 0\n",
    "CLS_ID = 1\n",
    "BOS_ID = 2\n",
    "EOS_ID = 3\n",
    "MSK_ID = 4\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "#           smiles_str2smiles      #\n",
    "#----------------------------------#\n",
    "# translate a string of SMILES to a list of symbol ID\n",
    "\n",
    "max_length_symbol = max([len(s) for s in ID_symbol])\n",
    "\n",
    "def smiles_str2smiles(smiles_str, flag=False):\n",
    "  \"translate a string of SMILES to a list of symbol ID（symbols such as 'Na' will translated to ID of 'Na')\n",
    "\n",
    "  smiles = []\n",
    "  i=0\n",
    "  while i < len(smiles_str):\n",
    "    NotFindID = True\n",
    "    for j in range(max_length_symbol,0,-1) :\n",
    "      if i+j <= len(smiles_str) and smiles_str[i:i+j] in symbol_ID: # \n",
    "        smiles.append(symbol_ID[smiles_str[i:i+j]])\n",
    "        i += j-1 \n",
    "        NotFindID = False\n",
    "        break\n",
    "    if NotFindID:\n",
    "      smiles.append(4) # using MSK_ID to replace unknown symbol\n",
    "    i += 1\n",
    "  return smiles\n",
    "\n",
    "#----------------------------------#\n",
    "#           smiles2smiles_str      #\n",
    "#----------------------------------#\n",
    "def smiles2smiles_str(smiles):\n",
    "  smiles_str = ''\n",
    "  for id in smiles:\n",
    "    smiles_str += ID_symbol[id]\n",
    "  return smiles_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logResult(filepath, filename, which_model, data_name, result_dict):\n",
    "    if os.path.exists(filepath):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    temp = os.path.isfile(filepath+filename)\n",
    "    \n",
    "    f = open(filepath+filename, 'a')\n",
    "    if temp == False:\n",
    "        title_list = [\n",
    "            'task_name', \n",
    "            'task_type',\n",
    "            'pretrain', \n",
    "            'macroF1 or RMSE',\n",
    "            'microF1 or r^2',\n",
    "            'ROCAUC_macro',\n",
    "            'ROCAUC_micro',\n",
    "            'PRAUC macro',\n",
    "            'PRAUC micro',\n",
    "            'tdc',\n",
    "            '\\n'\n",
    "        ]\n",
    "        f.write(', '.join(title_list))\n",
    "\n",
    "    if data_name in regression_list:\n",
    "        task_type = 'regression'\n",
    "    else:\n",
    "        task_type = 'classification'\n",
    "\n",
    "    content_list = [\n",
    "        data_name,\n",
    "        task_type,\n",
    "        which_model,\n",
    "        '%.4f' % result_dict['macroF1 or RMSE'],\n",
    "        '%.4f' % result_dict['microF1 or r^2'],\n",
    "        '%.4f' % result_dict['ROCAUC_macro'],\n",
    "        '%.4f' % result_dict['ROCAUC_micro'],\n",
    "        '%.4f' % result_dict['PRAUC macro'],\n",
    "        '%.4f' % result_dict['PRAUC micro'],\n",
    "        '%s' % result_dict['tdc'],\n",
    "        '\\n'\n",
    "    ]\n",
    "    f.write(', '.join(content_list))\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataMake\n",
    "* TDC: prepare for predicting molecular properties of TDC ADMET Benchmark Group\n",
    "* ODs: prepare for predicting odor descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tdc.benchmark_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "regression_list = [\n",
    "    'caco2_wang',\n",
    "    'lipophilicity_astrazeneca',\n",
    "    'solubility_aqsoldb',\n",
    "    'ppbr_az',\n",
    "    'vdss_lombardo',\n",
    "    'half_life_obach',\n",
    "    'clearance_hepatocyte_az',\n",
    "    'clearance_microsome_az',\n",
    "    'ld50_zhu',\n",
    "\n",
    "]\n",
    "\n",
    "classification_list = [\n",
    "    'hia_hou',\n",
    "    'pgp_broccatelli',\n",
    "    'bioavailability_ma',\n",
    "    'bbb_martins',\n",
    "    'cyp2d6_veith',\n",
    "    'cyp3a4_veith',\n",
    "    'cyp2c9_veith',\n",
    "    'cyp2c9_substrate_carbonmangels',\n",
    "    'cyp2d6_substrate_carbonmangels',\n",
    "    'cyp3a4_substrate_carbonmangels',\n",
    "    'herg',\n",
    "    'ames', \n",
    "    'dili', \n",
    "\n",
    "]\n",
    "\n",
    "data_name_list = regression_list + classification_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataMake_group(d_name, strSmiles_list, labels):\n",
    "    if d_name in regression_list:\n",
    "        task_type = 'regression'\n",
    "    else:\n",
    "        task_type = 'classification'\n",
    "\n",
    "    label_ID = {d_name: 0}\n",
    "    lID = 1\n",
    "    ID2label = [d_name]\n",
    "\n",
    "    smiles_list = []\n",
    "    labels_vec_list = []\n",
    "    canonical_smiles_list = []\n",
    "    for i in range(len(strSmiles_list)):\n",
    "        smi = strSmiles_list[i]\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol == None:\n",
    "            continue\n",
    "        else:\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            if len(smi) > LIMIT_SMILES_LENGTH:\n",
    "                smi = smi[:LIMIT_SMILES_LENGTH]\n",
    "                # continue\n",
    "        \n",
    "        temp = smiles_str2smiles(smi)\n",
    "        if temp is not None:\n",
    "            canonical_smiles_list.append(smi)\n",
    "            smiles_list.append(temp)\n",
    "\n",
    "            labels_vec_list.append([])\n",
    "            labels_vec_list[-1].append(float(labels[i]))\n",
    "        \n",
    "    return(\n",
    "        smiles_list, \n",
    "        canonical_smiles_list,\n",
    "        labels_vec_list,\n",
    "        lID,\n",
    "        label_ID,\n",
    "        task_type,\n",
    "        ID2label\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_odor     = 'webScrapping/tgsc_odorant_1020.txt'    # path of ODs data (refer to README.md about obtaining ODs data)       \n",
    "F_odorless = 'webScrapping/tgsc_odorless_1020.txt'  \n",
    "\n",
    "LIMIT_FREQ = 49   # predict ODs whose number of positive samples over LIMIT_FREQ \n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# compute frequence of ODs\n",
    "#----------------------------------------------------------------------------------------- \n",
    "label_ID ={}    \n",
    "freq_label = {} \n",
    "\n",
    "with open(F_odor,'r') as inF:\n",
    "  while True:\n",
    "    line = inF.readline()\n",
    "    if line == '':\n",
    "      break\n",
    "    # x = line.split(\"\\t\")\n",
    "    x = line.split()\n",
    "    smiles_str = x[1]\n",
    "\n",
    "    # the length of smiles_str longer than LMIT_SMILES_LENGTH will be omitted\n",
    "    if len(smiles_str) > LIMIT_SMILES_LENGTH :\n",
    "      continue\n",
    "    else:\n",
    "      mol = Chem.MolFromSmiles(smiles_str)\n",
    "      if mol == None:\n",
    "        continue\n",
    "      else:\n",
    "        # odor_descriptors = x[2].split()\n",
    "        odor_descriptors = x[2:]\n",
    "        for odd in odor_descriptors:\n",
    "          if odd == 'odorless':\n",
    "            continue\n",
    "          if odd in freq_label:\n",
    "            freq_label[odd] += 1\n",
    "          else:\n",
    "            freq_label[odd] = 1\n",
    "\n",
    "#----------------------------------------------\n",
    "# create ID for ODs\n",
    "#----------------------------------------------\n",
    "ID2label = []\n",
    "lID = 0\n",
    "\n",
    "freq_label_sorted = sorted(freq_label.items(), key=lambda x:x[1], reverse=True)\n",
    "for label, freq in freq_label_sorted:\n",
    "  if freq > LIMIT_FREQ:\n",
    "    label_ID[label] = lID\n",
    "    ID2label.append(label)\n",
    "    lID += 1\n",
    "\n",
    "print(\"number of odor descriptors: \", lID)\n",
    "for label, freq in freq_label_sorted:\n",
    "  if freq>49:\n",
    "     print('%5d : '%freq, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------\n",
    "#   make target (labels_vec_list)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def labels2vec(label_sequence):\n",
    "  # label_list = label_sequence.split()\n",
    "  label_list = label_sequence.copy()\n",
    "  labels_vec = [0.0]*lID\n",
    "  for label in label_list:\n",
    "    if label == 'odorless':  \n",
    "      continue\n",
    "    if label not in label_ID:\n",
    "      continue\n",
    "    labels_vec[label_ID[label]] = 1.0  \n",
    "  return labels_vec\n",
    "\n",
    "smiles_list = []\n",
    "labels_vec_list = []\n",
    "canonical_smiles_list = []\n",
    "\n",
    "def make_data(filename):\n",
    "  \"make smiles_list and labels_vec_list\"\n",
    "\n",
    "  with open(filename,'r') as inF:\n",
    "    while True:\n",
    "      line = inF.readline()\n",
    "      if line == '':\n",
    "        break\n",
    "\n",
    "      x = line.split()\n",
    "      smiles_str = x[1]\n",
    "\n",
    "      # the length of smiles_str longer than LMIT_SMILES_LENGTH will be omitted\n",
    "      if len(smiles_str) > LIMIT_SMILES_LENGTH :\n",
    "        continue\n",
    "      else:\n",
    "        mol = Chem.MolFromSmiles(smiles_str)\n",
    "        if mol == None:\n",
    "          continue\n",
    "        else:\n",
    "          # smiles_str to canonical smiles_str\n",
    "          canonical_smiles_str = Chem.MolToSmiles(mol)\n",
    "          smiles_list.append(smiles_str2smiles(canonical_smiles_str))      \n",
    "          labels_vec_list.append(labels2vec(x[2:])) \n",
    "          canonical_smiles_list.append(canonical_smiles_str)\n",
    "\n",
    "make_data(F_odor)\n",
    "make_data(F_odorless)\n",
    "\n",
    "print(smiles_list[0])\n",
    "print(labels_vec_list[0])\n",
    "print(smiles_list[1])\n",
    "print(labels_vec_list[1])\n",
    "task_type = 'classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitRange = 0.1\n",
    "NumToken = sID\n",
    "NumHead = 8\n",
    "Activation = 'gelu'\n",
    "NormFirst = True\n",
    "DimOdorCode = 100\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=2048):  # d_model: dimension of embedding\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) #.transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "class SymbolEncoder(nn.Module):\n",
    "    def __init__(self, num_token, d_model):  \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(num_token, d_model, padding_idx=PAD_ID)\n",
    "        self.embed.weight.data.uniform_(-InitRange, InitRange)  # embedding init\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embed(src) * math.sqrt(self.d_model)\n",
    "        return src\n",
    "#----------------------------------------------------------------------------\n",
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_head, d_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, num_head, dim_feedforward=d_hidden, norm_first = NormFirst, activation=Activation, dropout=Dropout, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NumLayers, norm=encoder_norm)     \n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask = padding_mask)\n",
    "        return x\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, lID, task_type):\n",
    "        super().__init__()\n",
    "        self.task_type = task_type\n",
    "\n",
    "        self.positional_encoder = PositionalEncoder(DimEmbed)\n",
    "        self.symbol_encoder = SymbolEncoder(NumToken, DimEmbed) \n",
    "        self.smiles_encoder = MyTransformerEncoder(DimEmbed, NumHead, DimTfHidden)\n",
    "        self.drop0 = nn.Dropout(p=Dropout)\n",
    "\n",
    "        self.drop = nn.Dropout(p=Dropout)  \n",
    "        self.fnn = nn.Linear(DimEmbed, lID)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "        self.drop2 = nn.Dropout(p=Dropout)\n",
    "        self.fnn2 = nn.Linear(DimEmbed, DimEmbed)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, smiles):\n",
    "        # add cls\n",
    "        cls = torch.ones(smiles.size(0),1,dtype=torch.long).to(device)\n",
    "        x = torch.concat((cls, smiles), dim=1) \n",
    "        padding_mask = (x == PAD_ID)\n",
    "        x = self.drop0(self.symbol_encoder(x) + self.positional_encoder(x)) \n",
    "        x = self.smiles_encoder(x, padding_mask)  # x : OdorCode\n",
    "\n",
    "        x = self.act2(self.fnn2(self.drop2(x[:,0,:])))\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "                return self.act(self.fnn(self.drop(x)))\n",
    "        else:\n",
    "            return self.fnn(self.drop(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mainProgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tdcGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def main_program(rate1, hpara, fold_cv=5, data_name='caco2_wang'):\n",
    "    '''\n",
    "    the learning rate for layers training in transfer learning is: LearningRate * rate1\n",
    "    hpara = {\n",
    "        'pretrain_path': 'modelsave0203',\n",
    "        'DimEmbed': 256,\n",
    "        'DimTfHidden': 512,\n",
    "        'NumHead': 8,\n",
    "        'NumLayers': 10,\n",
    "        'MaskRate': 0.5,\n",
    "        'LearningRate': 0.0001, \n",
    "        'Dropout': 0.1\n",
    "    } \n",
    "    \n",
    "    fold_cv is the number of seeds for train_val split\n",
    "    '''\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load data\n",
    "    tdc_group = tdc.benchmark_group.admet_group(path = '/tf/haha/all_data/TDC/admet_group/')\n",
    "    benchmark = tdc_group.get(data_name)\n",
    "    train_val, test_set = benchmark['train_val'], benchmark['test']\n",
    "    # test set\n",
    "    smiles_list_test, canonical_smiles_list_test,labels_vec_list_test,lID,label_ID,task_type,ID2label = dataMake_group(data_name, test_set.Drug.to_list(), test_set.Y.to_list())\n",
    "    print(len(smiles_list_test))\n",
    "\n",
    "    para_name = '%d_%d_%d_%d_%.2f_%.2f_%.2f' % (hpara['DimEmbed'], hpara['DimTfHidden'], \n",
    "    hpara['NumHead'], hpara['NumLayers'], hpara['MaskRate'], hpara['LearningRate'], hpara['Dropout'])\n",
    "\n",
    "    list_train_loss = []\n",
    "    list_train_metrics = [] # micro f1 or rmse\n",
    "    list_test_loss = []\n",
    "    list_test_metrics = [] # micro f1 or rmse\n",
    "\n",
    "    if task_type == 'classfication':\n",
    "        criterion_label = nn.BCELoss()\n",
    "    else:\n",
    "        criterion_label = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    fold_cv = fold_cv\n",
    "\n",
    "    # record result for tensorboard\n",
    "    # record F1 for classification and RMSE for regression\n",
    "    avgOD_metrics = torch.zeros(fold_cv, NumEpoch).to(device) # microF1 or rmse\n",
    "    eachODFold = torch.zeros(fold_cv, NumEpoch, lID).to(device) #macroF1 can be obtained by averaging lID axis\n",
    "\n",
    "    avgOD_metrics_test = torch.zeros(fold_cv, NumEpoch).to(device) # microF1 or rmse\n",
    "    eachODFold_test = torch.zeros(fold_cv, NumEpoch, lID).to(device) # macroF1 can be obtained by averaging lID axis\n",
    "\n",
    "    best_metrics = 0\n",
    "\n",
    "    # used for changing canonical SMILES to non-canonical SMILES\n",
    "    def change_smiles(smi):\n",
    "        if smi.find('.') >= 0:\n",
    "            return smiles_str2smiles(smi)\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            return smiles_str2smiles(smi)\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        pos_list = list(range(num_atoms))\n",
    "\n",
    "        pos = random.choice(pos_list)\n",
    "        new_smi = Chem.MolToSmiles(mol, rootedAtAtom=pos)\n",
    "        if len(new_smi) < LIMIT_SMILES_LENGTH:\n",
    "            return smiles_str2smiles(new_smi)\n",
    "        else:\n",
    "            return smiles_str2smiles(smi)\n",
    "\n",
    "    rocauc_macro = 0\n",
    "    pr_auc_macro = 0\n",
    "    rocauc_micro = 0\n",
    "    pr_auc_micro = 0\n",
    "    r_2 = 0\n",
    "\n",
    "    \n",
    "    tdc_predictions_list = []\n",
    "\n",
    "    for _fold in range(fold_cv):\n",
    "        print('seed = ', _fold)\n",
    "\n",
    "        # load data\n",
    "        train_set, valid_set = tdc_group.get_train_valid_split(benchmark =data_name, split_type = 'default', seed =_fold)\n",
    "        labels_train = train_set.Y.to_list()\n",
    "        smilesStr_list = train_set.Drug.to_list()\n",
    "        smiles_list, canonical_smiles_list,labels_vec_list,lID,label_ID,task_type,ID2label = dataMake_group(data_name, smilesStr_list, labels_train)\n",
    "\n",
    "        #============================= train_dataloader ===========================\n",
    "        train_dataloader = DataLoader(list(range(len(smiles_list))), batch_size, shuffle=True)\n",
    "\n",
    "        #============================== test_dataloader  =====================================\n",
    "        test_dataloader = DataLoader(list(range(len(smiles_list_test))), batch_size, shuffle=False)  \n",
    "\n",
    "         #====================== model, optimizer  ==============\n",
    "        label_estimator = Model(lID, task_type).to(device)\n",
    "\n",
    "        # load pretrained encoder\n",
    "\n",
    "        if hpara['pretrain_path'] == 'nopre':\n",
    "            optimizer_label_estimator = optim.Adam(label_estimator.parameters(), hpara['LearningRate'])\n",
    "        else:\n",
    "            pf_symbol_en = hpara['pretrain_path']+'/'+'OdorCode-40 symbol_encoder D'+str(hpara['DimEmbed'])+'.Hidden'+str(hpara['DimTfHidden'])+'.Head'+ str(hpara['NumHead'])+'.L'+str(hpara['NumLayers'])+'.R'+str(hpara['MaskRate'])+'.S'+str(TotalSize)+'-epoch.'+str(ModelEpoch)\n",
    "            pf_smiles_en = hpara['pretrain_path']+'/'+'OdorCode-40 smiles_encoder D'+str(hpara['DimEmbed'])+'.Hidden'+str(hpara['DimTfHidden'])+'.Head'+ str(hpara['NumHead'])+'.L'+str(hpara['NumLayers'])+'.R'+str(hpara['MaskRate'])+'.S'+str(TotalSize)+'-epoch.'+str(ModelEpoch)\n",
    "            print('reading '+pf_symbol_en)\n",
    "            label_estimator.symbol_encoder.load_state_dict(torch.load(pf_symbol_en))\n",
    "            print('reading '+pf_smiles_en)\n",
    "            label_estimator.smiles_encoder.load_state_dict(torch.load(pf_smiles_en))\n",
    "\n",
    "            for param in label_estimator.symbol_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in label_estimator.smiles_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in label_estimator.smiles_encoder.transformer_encoder.layers[-1].parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            optimizer_label_estimator = optim.Adam([ {'params': label_estimator.smiles_encoder.transformer_encoder.layers[-1].parameters(), 'lr': hpara['LearningRate']*rate1},\n",
    "                                                        {'params': label_estimator.fnn2.parameters(), 'lr': hpara['LearningRate']},\n",
    "                                                        {'params': label_estimator.fnn.parameters(), 'lr': hpara['LearningRate']}])\n",
    "            \n",
    "        if task_type == 'classification':\n",
    "            best_metrics_fold = 0 # macro f1\n",
    "        else:\n",
    "            best_metrics_fold = 1e9 # rmse\n",
    "        best_logits_pred_fold = []\n",
    "        best_epo_fold = None\n",
    "\n",
    "        \n",
    "        for epoch in range(NumEpoch):\n",
    "            #---------------- train step -----------------\n",
    "            sample_num = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if task_type == 'classification': \n",
    "                total_tps = torch.zeros(lID).to(device)\n",
    "                total_real_pos = torch.zeros(lID).to(device)\n",
    "                total_pred_pos = torch.zeros(lID).to(device)\n",
    "\n",
    "            label_estimator.train()\n",
    "\n",
    "            for idxs in train_dataloader:\n",
    "                smiles = pad_sequence([torch.tensor([BOS_ID]+change_smiles(canonical_smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                # smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                labels = torch.tensor([labels_vec_list[idx] for idx in idxs],dtype=torch.float).to(device)\n",
    "\n",
    "                optimizer_label_estimator.zero_grad()\n",
    "\n",
    "                estimated_labels = label_estimator(smiles)\n",
    "\n",
    "                loss = criterion_label(estimated_labels, labels)\n",
    "                \n",
    "                loss.backward()     \n",
    "\n",
    "                optimizer_label_estimator.step()  \n",
    "\n",
    "                total_loss += loss.item()*len(idxs)\n",
    "                sample_num += len(idxs)\n",
    "\n",
    "                if task_type == 'classification': \n",
    "                    total_tps += seek_TP(estimated_labels, labels).sum(0)\n",
    "                    total_real_pos += (positive(labels).int()).sum(0)\n",
    "                    total_pred_pos += (positive(estimated_labels).int()).sum(0)\n",
    "\n",
    "            mean_loss = total_loss / sample_num\n",
    "            list_train_loss.append(mean_loss)\n",
    "\n",
    "            # compute metrics\n",
    "            if task_type == 'classification':\n",
    "                precision = total_tps/(total_pred_pos + 1e-9)\n",
    "                recall    = total_tps/(total_real_pos + 1e-9)\n",
    "                f1 = 2.0*precision*recall/(precision+recall+1e-9) # (lID, )\n",
    "                eachODFold[_fold, epoch] = f1\n",
    "                micro_mean_precision = total_tps.sum()/total_pred_pos.sum()\n",
    "                micro_mean_recall    = total_tps.sum()/total_real_pos.sum()      \n",
    "                metrics = 2*micro_mean_precision*micro_mean_recall/(micro_mean_precision+micro_mean_recall)\n",
    "                metrics = metrics.item()\n",
    "            else:\n",
    "                metrics = math.sqrt(mean_loss)\n",
    "                eachODFold[_fold, epoch, 0] = metrics \n",
    "\n",
    "            list_train_metrics.append(metrics)\n",
    "\n",
    "            avgOD_metrics[_fold, epoch] = metrics # record for tensorboard\n",
    "            \n",
    "            # plot on tensorboard\n",
    "            writter.add_scalars('fold%s-microF1orRMSE/train' % _fold, {para_name: metrics}, epoch)\n",
    "            if _fold == fold_c-1:\n",
    "                macro_f1 = torch.sum(eachODFold[:, epoch, :]).item() / (fold_cv*lID)\n",
    "                writter.add_scalars('AvgFold-macroF1orRMSE/train', {para_name: macro_f1}, epoch) \n",
    "                for i in range(lID):\n",
    "                    od_f1 = torch.sum(eachODFold[:, epoch, i]).item() / fold_cv\n",
    "                    writter.add_scalars('AvgFold-%s/train' % ID2label[i], {para_name:od_f1}, epoch)\n",
    "\n",
    "\n",
    "            #--------------- test step ------------------\n",
    "            sample_num = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if task_type == 'classification':\n",
    "                total_tps = torch.zeros(lID).to(device)\n",
    "                total_real_pos = torch.zeros(lID).to(device)\n",
    "                total_pred_pos = torch.zeros(lID).to(device)\n",
    "\n",
    "            label_estimator.eval()\n",
    "\n",
    "            labels_test = torch.zeros([1, lID]).to(device)\n",
    "            preds_test = torch.zeros([1, lID]).to(device)\n",
    "\n",
    "            for idxs in test_dataloader:\n",
    "                smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list_test[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                labels = torch.tensor([labels_vec_list_test[idx] for idx in idxs],dtype=torch.float).to(device)\n",
    "\n",
    "                estimated_labels = label_estimator(smiles)\n",
    "\n",
    "                loss = criterion_label(estimated_labels, labels)\n",
    "\n",
    "                total_loss += loss.item()*len(idxs)\n",
    "                sample_num += len(idxs)\n",
    "\n",
    "                if task_type == 'classification': \n",
    "                    total_tps += seek_TP(estimated_labels, labels).sum(0)\n",
    "                    total_real_pos += (positive(labels).int()).sum(0)\n",
    "                    total_pred_pos += (positive(estimated_labels).int()).sum(0)\n",
    "\n",
    "                labels_test = torch.concat((labels_test, labels), dim=0)\n",
    "                preds_test = torch.concat((preds_test, estimated_labels), dim=0)\n",
    "\n",
    "            labels_test = labels_test[1:]\n",
    "            preds_test = preds_test[1:]\n",
    "\n",
    "            mean_loss = total_loss / sample_num\n",
    "            list_test_loss.append(mean_loss)\n",
    "\n",
    "            # compute metrics\n",
    "            if task_type == 'classification':\n",
    "                precision = total_tps/(total_pred_pos + 1e-9)\n",
    "                recall    = total_tps/(total_real_pos + 1e-9)\n",
    "                f1 = 2.0*precision*recall/(precision+recall+ 1e-9)\n",
    "                eachODFold_test[_fold, epoch] = f1\n",
    "                micro_mean_precision = total_tps.sum()/total_pred_pos.sum()\n",
    "                micro_mean_recall    = total_tps.sum()/total_real_pos.sum()     \n",
    "                metrics = 2*micro_mean_precision*micro_mean_recall/(micro_mean_precision+micro_mean_recall)\n",
    "                metrics = metrics.item()\n",
    "            else:\n",
    "                metrics = math.sqrt(mean_loss)\n",
    "                eachODFold_test[_fold, epoch, 0] = metrics\n",
    "\n",
    "            list_test_metrics.append(metrics)\n",
    "\n",
    "            avgOD_metrics_test[_fold, epoch] = metrics # record for tensorboard\n",
    "\n",
    "            # plot on tensorboard\n",
    "            writter.add_scalars('fold%s-microF1orRMSE/test' % _fold, {para_name: metrics}, epoch)\n",
    "            \n",
    "            macro_f1_fold = torch.sum(eachODFold_test[_fold, epoch, :]).item() / (lID)\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                if macro_f1_fold > best_metrics_fold:\n",
    "                    best_metrics_fold = macro_f1_fold\n",
    "                    best_logits_pred_fold = preds_test.tolist() # shape=(batch, lID)\n",
    "                    best_labels = labels_test.tolist()\n",
    "                    best_epo_fold = epoch\n",
    "            else:\n",
    "                if macro_f1_fold < best_metrics_fold:\n",
    "                    best_metrics_fold = macro_f1_fold\n",
    "                    best_logits_pred_fold = preds_test.tolist() # shape=(batch, lID)\n",
    "                    best_labels = labels_test.tolist()\n",
    "            \n",
    "            \n",
    "            if _fold == fold_cv-1:\n",
    "                macro_f1 = torch.sum(eachODFold_test[:, epoch, :]).item() / (fold_cv*lID)\n",
    "                writter.add_scalars('AvgFold-macroF1orRMSE/test', {para_name: macro_f1}, epoch) \n",
    "                f1_eachod = torch.sum(eachODFold_test[:,epoch,:], dim=0) / fold_cv # shape=(lID, )\n",
    "                for i in range(lID):\n",
    "                    # od_f1 = torch.sum(eachODFold_test[:, epoch, i]).item() / fold_cv\n",
    "                    writter.add_scalars('AvgFold-%s/test' % ID2label[i], {para_name: f1_eachod[i].item()}, epoch)\n",
    "\n",
    "        best_metrics += best_metrics_fold\n",
    "\n",
    "\n",
    "        # auc, r_square\n",
    "        if task_type == 'classification':\n",
    "            rocauc_macro += roc_auc_score(best_labels, best_logits_pred_fold, average='macro')\n",
    "            pr_auc_macro += average_precision_score(best_labels, best_logits_pred_fold, average='macro')\n",
    "            rocauc_micro += roc_auc_score(best_labels, best_logits_pred_fold, average='micro')\n",
    "            pr_auc_micro += average_precision_score(best_labels, best_logits_pred_fold, average='micro')\n",
    "            r_2 += avgOD_metrics_test[_fold, best_epo_fold].item()\n",
    "        else:\n",
    "            r_2 += r2_score(best_labels, best_logits_pred_fold)\n",
    "        tdc_predictions_list.append({data_name: best_logits_pred_fold})\n",
    "\n",
    "    \n",
    "    result_dict = {'macroF1 or RMSE': best_metrics/fold_cv, \n",
    "    'ROCAUC_macro': rocauc_macro/fold_cv, 'ROCAUC_micro': rocauc_micro/fold_cv,\n",
    "    'PRAUC macro': pr_auc_macro/fold_cv, 'PRAUC micro': pr_auc_micro/fold_cv, \n",
    "    'microF1 or r^2': r_2/fold_cv}\n",
    "\n",
    "    \n",
    "    writter.add_hparams(hpara,\n",
    "                            result_dict, \n",
    "                            run_name=para_name\n",
    "                            )\n",
    "\n",
    "    tdc_result = tdc_group.evaluate_many(tdc_predictions_list)\n",
    "    print(tdc_result)\n",
    "\n",
    "    result_dict['tdc'] = tdc_result[data_name]\n",
    "\n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def main_program_ODs(rate1, hpara):\n",
    "    '''\n",
    "    the learning rate for layers training in transfer learning is: LearningRate * rate1\n",
    "    hpara = {\n",
    "        'pretrain_path': 'modelsave0203',\n",
    "        'DimEmbed': 256,\n",
    "        'DimTfHidden': 512,\n",
    "        'NumHead': 8,\n",
    "        'NumLayers': 10,\n",
    "        'MaskRate': 0.5,\n",
    "        'LearningRate': 0.0001, \n",
    "        'Dropout': 0.1\n",
    "    } \n",
    "    '''\n",
    "    smiles_list, canonical_smiles_list,labels_vec_list,lID,label_ID,task_type,ID2label = dataTDC(data_name)\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    para_name = '%d_%d_%d_%d_%.2f_%.2f_%.2f' % (hpara['DimEmbed'], hpara['DimTfHidden'], \n",
    "    hpara['NumHead'], hpara['NumLayers'], hpara['MaskRate'], hpara['LearningRate'], hpara['Dropout'])\n",
    "\n",
    "    list_train_loss = []\n",
    "    list_train_metrics = [] # micro f1 or rmse\n",
    "    list_test_loss = []\n",
    "    list_test_metrics = [] # micro f1 or rmse\n",
    "\n",
    "    if task_type == 'classfication':\n",
    "        criterion_label = nn.BCELoss()\n",
    "    else:\n",
    "        criterion_label = nn.MSELoss()\n",
    "\n",
    "    fold_cv = 5\n",
    "    kf = KFold(n_splits=fold_cv, shuffle=True)\n",
    "\n",
    "\n",
    "    dataset_idxs = list(range(len(smiles_list)))\n",
    "\n",
    "    sum_train_micro_mean_f1 = 0 \n",
    "    sum_test_micro_mean_f1 = 0\n",
    "    num_lastpart = 0\n",
    "\n",
    "    # record result for tensorboard\n",
    "    # record F1 for classification and RMSE for regression\n",
    "    avgOD_metrics = torch.zeros(fold_cv, NumEpoch).to(device) # microF1 or rmse\n",
    "    eachODFold = torch.zeros(fold_cv, NumEpoch, lID).to(device) #macroF1 can be obtained by averaging lID axis\n",
    "\n",
    "    avgOD_metrics_test = torch.zeros(fold_cv, NumEpoch).to(device) # microF1 or rmse\n",
    "    eachODFold_test = torch.zeros(fold_cv, NumEpoch, lID).to(device) #macroF1 can be obtained by averaging lID axis\n",
    "\n",
    "\n",
    "    # if task_type == 'classification':\n",
    "    #     best_metrics = 0 # macro f1\n",
    "    # else:\n",
    "    #     best_metrics = 1e9 # rmse\n",
    "    best_metrics = 0\n",
    "    best_logits_pred = []\n",
    "\n",
    "    # 改换单个smiles的root\n",
    "    def change_smiles(smi):\n",
    "        if smi.find('.') >= 0:\n",
    "            return smiles_str2smiles(smi)\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        pos_list = list(range(num_atoms))\n",
    "\n",
    "        pos = random.choice(pos_list)\n",
    "        new_smi = Chem.MolToSmiles(mol, rootedAtAtom=pos)\n",
    "        if len(new_smi) < LIMIT_SMILES_LENGTH:\n",
    "            return smiles_str2smiles(new_smi)\n",
    "        else:\n",
    "            return smiles_str2smiles(smi)\n",
    "\n",
    "    rocauc_macro = 0\n",
    "    pr_auc_macro = 0\n",
    "    rocauc_micro = 0\n",
    "    pr_auc_micro = 0\n",
    "    r_2 = 0\n",
    "\n",
    "        \n",
    "    for _fold, (train_idxs, test_idxs) in enumerate(kf.split(dataset_idxs)):\n",
    "        #============================= train_dataloader ===========================\n",
    "        train_dataset_idxs = Subset(dataset_idxs, train_idxs)\n",
    "        train_dataloader = DataLoader(train_dataset_idxs, batch_size, shuffle=True)\n",
    "\n",
    "        #============================== test_dataloader =====================================\n",
    "        test_dataset_idxs  = Subset(dataset_idxs, test_idxs)\n",
    "        test_dataloader = DataLoader(test_dataset_idxs, batch_size, shuffle=True)  \n",
    "\n",
    "        #====================== モデルの取得、optimizer ==============\n",
    "        label_estimator = Model(lID, task_type).to(device)\n",
    "\n",
    "\n",
    "        # 事前学習済みの smiles_encoder のパラメタを読み込む\n",
    "        print('fold = ', _fold)\n",
    "\n",
    "        if hpara['pretrain_path'] == 'nopre':\n",
    "            optimizer_label_estimator = optim.Adam(label_estimator.parameters(), hpara['LearningRate'])\n",
    "        else:\n",
    "            pf_symbol_en = hpara['pretrain_path']+'/'+'OdorCode-40 symbol_encoder D'+str(hpara['DimEmbed'])+'.Hidden'+str(hpara['DimTfHidden'])+'.Head'+ str(hpara['NumHead'])+'.L'+str(hpara['NumLayers'])+'.R'+str(hpara['MaskRate'])+'.S'+str(TotalSize)+'-epoch.'+str(ModelEpoch)\n",
    "            pf_smiles_en = hpara['pretrain_path']+'/'+'OdorCode-40 smiles_encoder D'+str(hpara['DimEmbed'])+'.Hidden'+str(hpara['DimTfHidden'])+'.Head'+ str(hpara['NumHead'])+'.L'+str(hpara['NumLayers'])+'.R'+str(hpara['MaskRate'])+'.S'+str(TotalSize)+'-epoch.'+str(ModelEpoch)\n",
    "            print('reading '+pf_symbol_en)\n",
    "            label_estimator.symbol_encoder.load_state_dict(torch.load(pf_symbol_en))\n",
    "            print('reading '+pf_smiles_en)\n",
    "            label_estimator.smiles_encoder.load_state_dict(torch.load(pf_smiles_en))\n",
    "\n",
    "            for param in label_estimator.symbol_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in label_estimator.smiles_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in label_estimator.smiles_encoder.transformer_encoder.layers[-1].parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            optimizer_label_estimator = optim.Adam([ {'params': label_estimator.smiles_encoder.transformer_encoder.layers[-1].parameters(), 'lr': hpara['LearningRate']*rate1},\n",
    "                                                        {'params': label_estimator.fnn2.parameters(), 'lr': hpara['LearningRate']},\n",
    "                                                        {'params': label_estimator.fnn.parameters(), 'lr': hpara['LearningRate']}])\n",
    "                                                      {'params': label_estimator.classifier.parameters(), 'lr': hpara['LearningRate']}])\n",
    "\n",
    "        if task_type == 'classification':\n",
    "            best_metrics_fold = 0 # macro f1\n",
    "        else:\n",
    "            best_metrics_fold = 1e9 # rmse\n",
    "        best_logits_pred_fold = []\n",
    "        best_epo_fold = None\n",
    "\n",
    "        # rocauc_macro = 0\n",
    "        # pr_auc_macro = 0\n",
    "        # rocauc_micro = 0\n",
    "        # pr_auc_micro = 0\n",
    "        # r_2 = 0\n",
    "\n",
    "        for epoch in range(NumEpoch):\n",
    "            #---------------- train step -----------------\n",
    "            sample_num = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if task_type == 'classification': \n",
    "                total_tps = torch.zeros(lID).to(device)\n",
    "                total_real_pos = torch.zeros(lID).to(device)\n",
    "                total_pred_pos = torch.zeros(lID).to(device)\n",
    "\n",
    "            label_estimator.train()\n",
    "\n",
    "            for idxs in train_dataloader:\n",
    "                # change smiles root\n",
    "                smiles = pad_sequence([torch.tensor([BOS_ID]+change_smiles(canonical_smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                # smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                labels = torch.tensor([labels_vec_list[idx] for idx in idxs],dtype=torch.float).to(device)\n",
    "\n",
    "                optimizer_label_estimator.zero_grad()\n",
    "\n",
    "                estimated_labels = label_estimator(smiles)\n",
    "\n",
    "                loss = criterion_label(estimated_labels, labels)\n",
    "                \n",
    "                loss.backward()     \n",
    "\n",
    "                optimizer_label_estimator.step()  \n",
    "\n",
    "                total_loss += loss.item()*len(idxs)\n",
    "                sample_num += len(idxs)\n",
    "\n",
    "                if task_type == 'classification': \n",
    "                    total_tps += seek_TP(estimated_labels, labels).sum(0)\n",
    "                    total_real_pos += (positive(labels).int()).sum(0)\n",
    "                    total_pred_pos += (positive(estimated_labels).int()).sum(0)\n",
    "\n",
    "            mean_loss = total_loss / sample_num\n",
    "            list_train_loss.append(mean_loss)\n",
    "\n",
    "            # metrics计算\n",
    "            if task_type == 'classification':\n",
    "                precision = total_tps/(total_pred_pos + 1e-9)\n",
    "                recall    = total_tps/(total_real_pos + 1e-9)\n",
    "                f1 = 2.0*precision*recall/(precision+recall+1e-9) # (lID, )\n",
    "                eachODFold[_fold, epoch] = f1\n",
    "                micro_mean_precision = total_tps.sum()/total_pred_pos.sum()\n",
    "                micro_mean_recall    = total_tps.sum()/total_real_pos.sum()      \n",
    "                metrics = 2*micro_mean_precision*micro_mean_recall/(micro_mean_precision+micro_mean_recall)\n",
    "                metrics = metrics.item()\n",
    "            else:\n",
    "                metrics = math.sqrt(mean_loss)\n",
    "                eachODFold[_fold, epoch, 0] = metrics \n",
    "\n",
    "            list_train_metrics.append(metrics)\n",
    "\n",
    "            avgOD_metrics[_fold, epoch] = metrics # record for tensorboard\n",
    "            \n",
    "            # plot on tensorboard\n",
    "            writter.add_scalars('fold%s-microF1orRMSE/train' % _fold, {para_name: metrics}, epoch)\n",
    "            if _fold == fold_cv-1:\n",
    "                macro_f1 = torch.sum(eachODFold[:, epoch, :]).item() / (fold_cv*lID)\n",
    "                writter.add_scalars('AvgFold-macroF1orRMSE/train', {para_name: macro_f1}, epoch) \n",
    "                for i in range(lID):\n",
    "                    od_f1 = torch.sum(eachODFold[:, epoch, i]).item() / fold_cv\n",
    "                    writter.add_scalars('AvgFold-%s/train' % ID2label[i], {para_name:od_f1}, epoch)\n",
    "\n",
    "\n",
    "            #--------------- test step ------------------\n",
    "            sample_num = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if task_type == 'classification':\n",
    "                total_tps = torch.zeros(lID).to(device)\n",
    "                total_real_pos = torch.zeros(lID).to(device)\n",
    "                total_pred_pos = torch.zeros(lID).to(device)\n",
    "\n",
    "            label_estimator.eval()\n",
    "\n",
    "            labels_test = torch.zeros([1, lID]).to(device)\n",
    "            preds_test = torch.zeros([1, lID]).to(device)\n",
    "\n",
    "            for idxs in test_dataloader:\n",
    "                smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                labels = torch.tensor([labels_vec_list[idx] for idx in idxs],dtype=torch.float).to(device)\n",
    "\n",
    "                estimated_labels = label_estimator(smiles)\n",
    "\n",
    "                loss = criterion_label(estimated_labels, labels)\n",
    "\n",
    "                total_loss += loss.item()*len(idxs)\n",
    "                sample_num += len(idxs)\n",
    "\n",
    "                if task_type == 'classification': \n",
    "                    total_tps += seek_TP(estimated_labels, labels).sum(0)\n",
    "                    total_real_pos += (positive(labels).int()).sum(0)\n",
    "                    total_pred_pos += (positive(estimated_labels).int()).sum(0)\n",
    "\n",
    "                labels_test = torch.concat((labels_test, labels), dim=0)\n",
    "                preds_test = torch.concat((preds_test, estimated_labels), dim=0)\n",
    "\n",
    "            labels_test = labels_test[1:]\n",
    "            preds_test = preds_test[1:]\n",
    "\n",
    "            mean_loss = total_loss / sample_num\n",
    "            list_test_loss.append(mean_loss)\n",
    "\n",
    "            # metrics\n",
    "            if task_type == 'classification':\n",
    "                precision = total_tps/(total_pred_pos + 1e-9)\n",
    "                recall    = total_tps/(total_real_pos + 1e-9)\n",
    "                f1 = 2.0*precision*recall/(precision+recall+ 1e-9)\n",
    "                eachODFold_test[_fold, epoch] = f1\n",
    "                micro_mean_precision = total_tps.sum()/total_pred_pos.sum()\n",
    "                micro_mean_recall    = total_tps.sum()/total_real_pos.sum()     \n",
    "                metrics = 2*micro_mean_precision*micro_mean_recall/(micro_mean_precision+micro_mean_recall)\n",
    "                metrics = metrics.item()\n",
    "            else:\n",
    "                metrics = math.sqrt(mean_loss)\n",
    "                eachODFold_test[_fold, epoch, 0] = metrics\n",
    "\n",
    "            list_test_metrics.append(metrics)\n",
    "\n",
    "            avgOD_metrics_test[_fold, epoch] = metrics # record for tensorboard\n",
    "\n",
    "            # plot on tensorboard\n",
    "            writter.add_scalars('fold%s-microF1orRMSE/test' % _fold, {para_name: metrics}, epoch)\n",
    "            \n",
    "            macro_f1_fold = torch.sum(eachODFold_test[_fold, epoch, :]).item() / (lID)\n",
    "            \n",
    "            if task_type == 'classification':\n",
    "                if macro_f1_fold > best_metrics_fold:\n",
    "                    best_metrics_fold = macro_f1_fold\n",
    "                    best_logits_pred_fold = preds_test.tolist() # shape=(batch, lID)\n",
    "                    best_labels = labels_test.tolist()\n",
    "                    best_epo_fold = epoch\n",
    "            else:\n",
    "                if macro_f1_fold < best_metrics_fold:\n",
    "                    best_metrics_fold = macro_f1_fold\n",
    "                    best_logits_pred_fold = preds_test.tolist() # shape=(batch, lID)\n",
    "                    best_labels = labels_test.tolist()\n",
    "            \n",
    "            \n",
    "            if _fold == fold_cv-1:\n",
    "                macro_f1 = torch.sum(eachODFold_test[:, epoch, :]).item() / (fold_cv*lID)\n",
    "                writter.add_scalars('AvgFold-macroF1orRMSE/test', {para_name: macro_f1}, epoch) \n",
    "                f1_eachod = torch.sum(eachODFold_test[:,epoch,:], dim=0) / fold_cv # shape=(lID, )\n",
    "                for i in range(lID):\n",
    "                    # od_f1 = torch.sum(eachODFold_test[:, epoch, i]).item() / fold_cv\n",
    "                    writter.add_scalars('AvgFold-%s/test' % ID2label[i], {para_name: f1_eachod[i].item()}, epoch)\n",
    "\n",
    "                \n",
    "\n",
    "        best_metrics += best_metrics_fold\n",
    "            \n",
    "        if fold_1:\n",
    "            break\n",
    "\n",
    "\n",
    "        # auc, r_square\n",
    "        if task_type == 'classification':\n",
    "            rocauc_macro += roc_auc_score(best_labels, best_logits_pred_fold, average='macro')\n",
    "            pr_auc_macro += average_precision_score(best_labels, best_logits_pred_fold, average='macro')\n",
    "            rocauc_micro += roc_auc_score(best_labels, best_logits_pred_fold, average='micro')\n",
    "            pr_auc_micro += average_precision_score(best_labels, best_logits_pred_fold, average='micro')\n",
    "            r_2 += avgOD_metrics_test[_fold, best_epo_fold].item()\n",
    "        else:\n",
    "            r_2 += r2_score(best_labels, best_logits_pred_fold)\n",
    "\n",
    "    \n",
    "    result_dict = {'macroF1 or RMSE': best_metrics/fold_cv, \n",
    "    'ROCAUC_macro': rocauc_macro/fold_cv, 'ROCAUC_micro': rocauc_micro/fold_cv,\n",
    "    'PRAUC macro': pr_auc_macro/fold_cv, 'PRAUC micro': pr_auc_micro/fold_cv, \n",
    "    'microF1 or r^2': r_2/fold_cv}\n",
    "\n",
    "    \n",
    "    writter.add_hparams(hpara,\n",
    "                            result_dict, \n",
    "                            run_name=para_name\n",
    "                            )\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelsave0203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "hpara = {\n",
    "        'pretrain_path': 'modelsave0203',\n",
    "        'DimEmbed': 256,\n",
    "        'DimTfHidden': 512,\n",
    "        'NumHead': 8,\n",
    "        'NumLayers': 10,\n",
    "        'MaskRate': 0.5,\n",
    "        'LearningRate': 0.0001, #learning rate for transfer learning instead of pretraining\n",
    "        'Dropout': 0.1\n",
    "    } \n",
    "\n",
    "# hyperparameters for pretraining (used for loading pretrained model)\n",
    "DimEmbed = hpara['DimEmbed']     # dimension of embedding\n",
    "DimTfHidden = hpara['DimTfHidden']  # dimension of FNN in TransformerEncoder \n",
    "NumHead = hpara['NumHead']        #the number of heads of multi-head in  TransformerEncoder\n",
    "NumLayers = hpara['NumLayers']      #the number of layers of TransformerEncoder\n",
    "NormFirst = True   # Vision Transformer \n",
    "Activation = 'gelu' \n",
    "\n",
    "MaskRate = hpara['MaskRate']\n",
    "TotalSize = 100000\n",
    "ModelEpoch = 600\n",
    "\n",
    "# hyperparameters for transfer learning\n",
    "LearningRate = hpara['LearningRate']\n",
    "NumEpoch = 50\n",
    "Dropout = hpara['Dropout']\n",
    "batch_size = 32\n",
    "InitRange = 0.1\n",
    "\n",
    "# TDC prediction\n",
    "for data_name in data_name_list:\n",
    "    # writter\n",
    "    record_dict = 'tensorboard_logs/torch/240329/%s/' % data_name\n",
    "    writter = SummaryWriter(record_dict)\n",
    "\n",
    "    result_dict = main_program(1.0, hpara, fold_cv=5, data_name=data_name)\n",
    "\n",
    "    writter.close()\n",
    "\n",
    "    print(result_dict)\n",
    "\n",
    "    #csv\n",
    "    file_txt = 'resultTXT/240329/'\n",
    "    file_name = 'tdcG-enc.csv'\n",
    "    logResult(file_txt, \n",
    "                file_name, \n",
    "                hpara['pretrain_path'],\n",
    "                data_name,\n",
    "                result_dict\n",
    "                )\n",
    "\n",
    "# ODs prediction\n",
    "# # writter\n",
    "# record_dict = 'tensorboard_logs/torch/ODs/'\n",
    "# writter = SummaryWriter(record_dict)\n",
    "\n",
    "# main_program_ODs(1.0, hpara)\n",
    "\n",
    "# writter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimEmbed_list = [128,256]\n",
    "NumHead_list = [8]\n",
    "NumLayers_list = [8, 10]\n",
    "Dropout_list = [0.1]\n",
    "\n",
    "for DimEmbed in DimEmbed_list:\n",
    "    DimTfHidden = DimEmbed\n",
    "    for NumHead in NumHead_list:\n",
    "        for NumLayers in NumLayers_list:\n",
    "            for Dropout in Dropout_list:\n",
    "                hpara = {\n",
    "                    'pretrain_path': 'nopre',\n",
    "                    'DimEmbed': DimEmbed,\n",
    "                    'DimTfHidden': DimTfHidden,\n",
    "                    'NumHead': NumHead,\n",
    "                    'NumLayers': NumLayers,\n",
    "                    'MaskRate': 0.5,\n",
    "                    'LearningRate': 0.0003,\n",
    "                    'Dropout': Dropout\n",
    "                } \n",
    "\n",
    "                NormFirst = True   # Vision Transformer \n",
    "                Activation = 'gelu' \n",
    "                \n",
    "                LearningRate = hpara['LearningRate']\n",
    "                NumEpoch = 150\n",
    "                batch_size = 64\n",
    "                InitRange = 0.1\n",
    "\n",
    "                # TDC prediction\n",
    "                for data_name in data_name_list:\n",
    "                    # writter\n",
    "                    record_dict = 'tensorboard_logs/torch/240409-nopre/%s/' % data_name\n",
    "                    writter = SummaryWriter(record_dict)\n",
    "\n",
    "                    result_dict = main_program(1.0, hpara, fold_cv=1, data_name=data_name)\n",
    "\n",
    "                    writter.close()\n",
    "\n",
    "                    print(result_dict)\n",
    "\n",
    "                    #csv\n",
    "                    model_name = hpara['pretrain_path']+'%d_%d_%d_%d' % (hpara['DimEmbed'], hpara['DimTfHidden'], hpara['NumHead'], hpara['NumLayers'])\n",
    "\n",
    "                    file_txt = 'resultTXT/240409/'\n",
    "                    file_name = 'tdc_nonpretrain.csv'\n",
    "                    logResult(file_txt, \n",
    "                                file_name, \n",
    "                                model_name,\n",
    "                                data_name,\n",
    "                                result_dict\n",
    "                                ) \n",
    "\n",
    "                # ODs prediction\n",
    "                # # writter\n",
    "                # record_dict = 'tensorboard_logs/torch/ODs-nonpretrain/'\n",
    "                # writter = SummaryWriter(record_dict)\n",
    "\n",
    "                # main_program_ODs(1.0, hpara)\n",
    "\n",
    "                # writter.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.10.14 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
