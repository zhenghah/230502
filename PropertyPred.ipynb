{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read symbol dictionary\n",
    "the symbol dictionary is saved as 'OdorCode-40 Symbol Dictionary' by running 'datasetMake_pretrain.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_SMILES_LENGTH = 100\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open('CHEMBL/OdorCode-40 Symbol Dictionary', 'rb')\n",
    "[symbol_ID, ID_symbol, sID] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "PAD_ID = 0\n",
    "CLS_ID = 1\n",
    "BOS_ID = 2\n",
    "EOS_ID = 3\n",
    "MSK_ID = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some functions\n",
    "(1) positive, negative, seek_TP, seek_TN: functions used to compute F1\n",
    "\n",
    "(2) smiles_str2smiles: translate a SMILES to a list of symbols ID\n",
    "\n",
    "(3) smiles2smiles_str: translate a list of symbols ID to a SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = torch.tensor([0.5]).to(device) # 0か1かを分ける閾値を作成\n",
    "\n",
    "def positive(proba):\n",
    "    a = (proba >= Threshold) # 閾値未満でFalse、以上で True に変換\n",
    "    return a\n",
    "\n",
    "def negative(proba):\n",
    "    a = (proba < Threshold) # 閾値未満で True、以上で False に変換\n",
    "    return a\n",
    "\n",
    "def seek_TP(pred_y, y):\n",
    "  tp = (positive(pred_y) & positive(y)).int()\n",
    "  return tp\n",
    "\n",
    "def seek_TN(pred_y, y):\n",
    "  tn = (negative(pred_y) & negative(y)).int()\n",
    "  return tn\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "#           smiles_str2smiles      #\n",
    "#----------------------------------#\n",
    "# smiles を記号（1文字記号だけでなく，2文字，3文字記号も含む）のＩＤのリストに変換\n",
    "\n",
    "max_length_symbol = max([len(s) for s in ID_symbol])\n",
    "\n",
    "def smiles_str2smiles(smiles_str, flag=False):\n",
    "  \"smiles を記号の列に変換（長さ2のNaなどの元素記号も1つのindexに変換）\"\n",
    "\n",
    "  smiles = []\n",
    "  i=0\n",
    "  while i < len(smiles_str):\n",
    "    NotFindID = True\n",
    "    for j in range(max_length_symbol,0,-1) :\n",
    "      if i+j <= len(smiles_str) and smiles_str[i:i+j] in symbol_ID: # j長さの文字記号として辞書登録済み\n",
    "        smiles.append(symbol_ID[smiles_str[i:i+j]])\n",
    "        i += j-1 # while ブロックの最後で i++ されるが，j文字なので余分に i+=j-1\n",
    "        NotFindID = False\n",
    "        break\n",
    "    if NotFindID:\n",
    "      print('something wrong on converting smiles_str to smiles')\n",
    "      break\n",
    "    i += 1\n",
    "  return smiles\n",
    "\n",
    "#----------------------------------#\n",
    "#           smiles2smiles_str      #\n",
    "#----------------------------------#\n",
    "def smiles2smiles_str(smiles):\n",
    "  smiles_str = ''\n",
    "  for id in smiles:\n",
    "    smiles_str += ID_symbol[id]\n",
    "  return smiles_str\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process the data set of odor descriptors\n",
    "dataset used is same to our previous study, see paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_odor     = 'webScrapping/tgsc_odorant_1020.txt'           # 匂いのある物質の SMILES のファイル（実は１つだけ odorless）\n",
    "F_odorless = 'webScrapping/tgsc_odorless_1020.txt'  # 匂いのない物質の SMILES のファイル\n",
    "\n",
    "LIMIT_FREQ = 49   # 正例の数の下限\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# LIMIT_SMILES_LENGTH 以下の smiles を対象に付与されている odor descriptors の頻度を求める\n",
    "#----------------------------------------------------------------------------------------- \n",
    "label_ID ={}    # label(Odor Descriptor) のIDの辞書\n",
    "freq_label = {} # label の頻度情報の辞書\n",
    "\n",
    "with open(F_odor,'r') as inF:\n",
    "  while True:\n",
    "    line = inF.readline()\n",
    "    if line == '':\n",
    "      break\n",
    "    # x = line.split(\"\\t\")\n",
    "    x = line.split()\n",
    "    smiles_str = x[1]\n",
    "\n",
    "    # smiles_str が LMIT_SMILES_LENGTH より長いか mol に変換できなければパス\n",
    "    if len(smiles_str) > LIMIT_SMILES_LENGTH :\n",
    "      continue\n",
    "    else:\n",
    "      mol = Chem.MolFromSmiles(smiles_str)\n",
    "      if mol == None:\n",
    "        continue\n",
    "      else:\n",
    "        # odor_descriptors = x[2].split()\n",
    "        odor_descriptors = x[2:]\n",
    "        for odd in odor_descriptors:\n",
    "          if odd == 'odorless':\n",
    "            continue\n",
    "          if odd in freq_label:\n",
    "            freq_label[odd] += 1\n",
    "          else:\n",
    "            freq_label[odd] = 1\n",
    "\n",
    "#----------------------------------------------\n",
    "# LIMIT_FREQ 以上の頻度のラベルだけ ID 化する\n",
    "#----------------------------------------------\n",
    "ID2label = []\n",
    "lID = 0\n",
    "\n",
    "freq_label_sorted = sorted(freq_label.items(), key=lambda x:x[1], reverse=True)\n",
    "for label, freq in freq_label_sorted:\n",
    "  if freq > LIMIT_FREQ:\n",
    "    label_ID[label] = lID\n",
    "    ID2label.append(label)\n",
    "    lID += 1\n",
    "\n",
    "print(\"number of odor descriptors: \", lID)\n",
    "for label, freq in freq_label_sorted:\n",
    "  if freq>49:\n",
    "     print('%5d : '%freq, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------\n",
    "#   SMILESのリスト smiles_list と \n",
    "#   頻度条件を満たす odor descriptors に対する正例／負例を表すベクトルのリスト labels_vec_list の作成\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# スペースで区切られた label の列(label_sequence)を multi-category 分類用のベクトル(labels_vec)に変換\n",
    "def labels2vec(label_sequence):\n",
    "  # label_list = label_sequence.split()\n",
    "  label_list = label_sequence.copy()\n",
    "  labels_vec = [0.0]*lID\n",
    "  for label in label_list:\n",
    "    if label == 'odorless':  # odorless は labels_vec の成分にない。つまりすべての label の負例\n",
    "      continue\n",
    "    if label not in label_ID:\n",
    "      continue\n",
    "    labels_vec[label_ID[label]] = 1.0  \n",
    "  return labels_vec\n",
    "\n",
    "smiles_list = []\n",
    "labels_vec_list = []\n",
    "canonical_smiles_list = []\n",
    "\n",
    "def make_data(filename):\n",
    "  \"smiles_list と labels_vec_list を作成\"\n",
    "\n",
    "  with open(filename,'r') as inF:\n",
    "    while True:\n",
    "      line = inF.readline()\n",
    "      if line == '':\n",
    "        break\n",
    "\n",
    "      x = line.split()\n",
    "      smiles_str = x[1]\n",
    "\n",
    "      # smiles_str が LMIT_SMILES_LENGTH より長いか mol に変換できなければパス\n",
    "      if len(smiles_str) > LIMIT_SMILES_LENGTH :\n",
    "        continue\n",
    "      else:\n",
    "        mol = Chem.MolFromSmiles(smiles_str)\n",
    "        if mol == None:\n",
    "          continue\n",
    "        else:\n",
    "          # smiles_str を canonical に変換\n",
    "          canonical_smiles_str = Chem.MolToSmiles(mol)\n",
    "          smiles_list.append(smiles_str2smiles(canonical_smiles_str))      # canonical_smiles_str をID化してリストに追加\n",
    "          labels_vec_list.append(labels2vec(x[2:])) # ベクトル化した odor descriptors をリストに追加\n",
    "          canonical_smiles_list.append(canonical_smiles_str)\n",
    "\n",
    "make_data(F_odor)\n",
    "make_data(F_odorless)\n",
    "\n",
    "print(smiles_list[0])\n",
    "print(labels_vec_list[0])\n",
    "print(smiles_list[1])\n",
    "print(labels_vec_list[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read molculeNET data\n",
    "files of datasets were downloaded from https://moleculenet.org/datasets-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['ESOL', 'FreeSolv', 'Lipo', 'BACE', 'BBBP', 'Tox21']\n",
    "\n",
    "smiles_col = {'ESOL': -1, 'FreeSolv': 1, 'Lipo': 2, 'BACE': 0, 'BBBP': 3, 'Tox21': -1}\n",
    "\n",
    "target_col = {'ESOL': [-2], \n",
    "            'FreeSolv': [2], \n",
    "            'Lipo': [1], \n",
    "            'BACE': [2], \n",
    "            'BBBP': [2],\n",
    "            'Tox21': list(range(12))}\n",
    "\n",
    "label_name = {\n",
    "            'ESOL': ['ESOL'], \n",
    "            'FreeSolv': ['FreeSolv'], \n",
    "            'Lipo': ['Lipo'], \n",
    "            'BACE': ['BACE'], \n",
    "            'BBBP': ['BBBP'],\n",
    "            'Tox21': ['NR-AR','NR-AR-LBD','NR-AhR','NR-Aromatase','NR-ER','NR-ER-LBD','NR-PPAR-gamma','SR-ARE','SR-ATAD5','SR-HSE','SR-MMP','SR-p53']\n",
    "}\n",
    "\n",
    "def read_file(which_data, filepath=None):\n",
    "    if filepath is None:\n",
    "        filepath = 'all_data/moleculeNET/'+which_data+'.csv'\n",
    "\n",
    "    f = open(filepath)\n",
    "    all_line = f.read().split('\\n')[1:-1]\n",
    "    f.close()\n",
    "\n",
    "    smiles_list = []\n",
    "    labels_vec_list = []\n",
    "    lID = len(target_col[which_data])\n",
    "    canonical_smiles_list = []\n",
    "\n",
    "    label_ID = {}\n",
    "    temp = 0\n",
    "    for i in label_name[which_data]:\n",
    "        label_ID[i] = temp\n",
    "        temp += 1\n",
    "\n",
    "    if which_data in dataset_list[-3:]:\n",
    "        task_type = 'classification'\n",
    "        freq_dict = {}\n",
    "    else:\n",
    "        task_type = 'regression'\n",
    "        freq_dict = None\n",
    "\n",
    "    for each in all_line:\n",
    "        line = each.split(',')\n",
    "\n",
    "        smi = line[smiles_col[which_data]]\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol == None:\n",
    "            continue\n",
    "        else:\n",
    "            smi = Chem.MolToSmiles(mol)\n",
    "            if len(smi) > LIMIT_SMILES_LENGTH:\n",
    "                continue\n",
    "\n",
    "        canonical_smiles_list.append(smi)\n",
    "        smiles_list.append(smiles_str2smiles(smi))\n",
    "\n",
    "        labels_vec_list.append([])\n",
    "        for i in target_col[which_data]:\n",
    "            if line[i] == '':\n",
    "                line[i] = -1\n",
    "            labels_vec_list[-1].append(float(line[i]))\n",
    "    if task_type == 'classification':\n",
    "        for i in range(lID):\n",
    "            temp_name = label_name[which_data][i]\n",
    "            freq_dict[temp_name] = 0\n",
    "            for s in labels_vec_list:\n",
    "                if s[i] == 1:\n",
    "                    freq_dict[temp_name] += 1\n",
    "    \n",
    "    return (canonical_smiles_list, smiles_list, labels_vec_list, lID, label_ID, freq_dict, task_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_smiles_list, smiles_list, labels_vec_list, lID, label_ID, freq_dict, task_type = read_file('ESOL')\n",
    "ID2label = [i for i in label_ID]\n",
    "print(len(smiles_list))\n",
    "print(smiles_list[0])\n",
    "print(labels_vec_list[0])\n",
    "print(lID)\n",
    "print(label_ID)\n",
    "print(freq_dict)\n",
    "print(task_type)\n",
    "print(ID2label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitRange = 0.1\n",
    "NumToken = sID\n",
    "NumHead = 8\n",
    "Activation = 'gelu'\n",
    "NormFirst = True\n",
    "DimOdorCode = 100\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=2048):  # d_model: 1記号のembeddingの次元, max_len : 最大のbatchサイズ？\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) #.transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "class SymbolEncoder(nn.Module):\n",
    "    def __init__(self, num_token, d_model):  \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(num_token, d_model, padding_idx=PAD_ID)\n",
    "        self.embed.weight.data.uniform_(-InitRange, InitRange)  # embedding の再初期化\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embed(src) * math.sqrt(self.d_model)\n",
    "        return src\n",
    "#----------------------------------------------------------------------------\n",
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_head, d_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, num_head, dim_feedforward=d_hidden, norm_first = NormFirst, activation=Activation, dropout=Dropout, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NumLayers, norm=encoder_norm)     \n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask = padding_mask)\n",
    "        return x\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.positional_encoder = PositionalEncoder(DimEmbed)\n",
    "        self.symbol_encoder = SymbolEncoder(NumToken, DimEmbed) # ＊＊＊＊　symbol_encoder が１つで良いのか検討？　＊＊＊＊＊\n",
    "        self.smiles_encoder = MyTransformerEncoder(DimEmbed, NumHead, DimTfHidden)\n",
    "        self.drop0 = nn.Dropout(p=Dropout)\n",
    "\n",
    "        self.drop = nn.Dropout(p=Dropout)  \n",
    "        self.fnn = nn.Linear(DimEmbed, lID)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "        self.drop2 = nn.Dropout(p=Dropout)\n",
    "        self.fnn2 = nn.Linear(DimEmbed, DimEmbed)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, smiles):\n",
    "        # 先頭に cls を挿入\n",
    "        cls = torch.ones(smiles.size(0),1,dtype=torch.long).to(device)\n",
    "        x = torch.concat((cls, smiles), dim=1)  # 先頭に cls ('_', ID 1) を挿入\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        x = self.drop0(self.symbol_encoder(x) + self.positional_encoder(x)) \n",
    "        x = self.smiles_encoder(x, padding_mask)  # x : OdorCode\n",
    "\n",
    "        x = self.act2(self.fnn2(self.drop2(x[:,0,:])))\n",
    "\n",
    "        if task_type == 'classification':\n",
    "                return self.act(self.fnn(self.drop(x)))\n",
    "        else:\n",
    "            return self.fnn(self.drop(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for conducting experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def main_program(rate1, hpara):\n",
    "    '''\n",
    "    rate1 は　transformer 最終層の学習係数 = LearningRate * rate1 　を意味\n",
    "    hpara = {\n",
    "        'pretrain_path': modelsave,\n",
    "        'DimEmbed': 256,\n",
    "        'DimTfHidden': 256,\n",
    "        'NumHead': 8,\n",
    "        'NumLayers': 10,\n",
    "        'MaskRate': 0.5,\n",
    "        # 'ModelEpoch': 1000,\n",
    "        'LearningRate': 0.0003,\n",
    "        'Dropout': 0.3\n",
    "    } \n",
    "    '''\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    para_name = '%d_%d_%d_%d_%.2f_%.2f_%.2f' % (hpara['DimEmbed'], hpara['DimTfHidden'], \n",
    "    hpara['NumHead'], hpara['NumLayers'], hpara['MaskRate'], hpara['LearningRate'], hpara['Dropout'])\n",
    "\n",
    "    list_train_loss = []\n",
    "    list_train_metrics = [] # micro f1 or rmse\n",
    "    list_test_loss = []\n",
    "    list_test_metrics = [] # micro f1 or rmse\n",
    "\n",
    "    if task_type == 'classfication':\n",
    "        criterion_label = nn.BCELoss()\n",
    "    else:\n",
    "        criterion_label = nn.MSELoss()\n",
    "\n",
    "    fold_cv = 5\n",
    "    kf = KFold(n_splits=fold_cv, shuffle=True)\n",
    "\n",
    "    dataset_idxs = list(range(len(smiles_list)))\n",
    "\n",
    "    # tensorboard用数据\n",
    "    # 分类时记录的是F1, 回归时记录RMSE\n",
    "    avgOD_metrics = torch.zeros(fold_cv, NumEpoch).to(device) # 记录的是microF1或者rmse\n",
    "    eachODFold = torch.zeros(fold_cv, NumEpoch, lID).to(device) #mean(lID)可得macro F1\n",
    "\n",
    "    avgOD_metrics_test = torch.zeros(fold_cv, NumEpoch).to(device) # 记录的是microF1或者rmse\n",
    "    eachODFold_test = torch.zeros(fold_cv, NumEpoch, lID).to(device) #mean(lID)可得macro F1\n",
    "\n",
    "\n",
    "    if task_type == 'classification':\n",
    "        best_metrics = [0 for i in range(fold_cv)] # macro f1\n",
    "    else:\n",
    "        best_metrics = [1e9 for i in range(fold_cv)] # rmse\n",
    "    best_logits_pred = [None for i in range(fold_cv)]\n",
    "    best_labels = [None for i in range(fold_cv)]\n",
    "\n",
    "\n",
    "    # 改换单个smiles的root\n",
    "    def change_smiles(smi):\n",
    "        if smi.find('.') >= 0:\n",
    "            return smiles_str2smiles(smi)\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        pos_list = list(range(num_atoms))\n",
    "\n",
    "        pos = random.choice(pos_list)\n",
    "        new_smi = Chem.MolToSmiles(mol, rootedAtAtom=pos)\n",
    "        if len(new_smi) < LIMIT_SMILES_LENGTH:\n",
    "            return smiles_str2smiles(new_smi)\n",
    "        else:\n",
    "            return smiles_str2smiles(smi)\n",
    "        \n",
    "    for _fold, (train_idxs, test_idxs) in enumerate(kf.split(dataset_idxs)):\n",
    "        #============================= train_dataloader の設定 ===========================\n",
    "        train_dataset_idxs = Subset(dataset_idxs, train_idxs)\n",
    "        train_dataloader = DataLoader(train_dataset_idxs, batch_size, shuffle=True)\n",
    "\n",
    "        #============================== test_dataloader の設定 =====================================\n",
    "        test_dataset_idxs  = Subset(dataset_idxs, test_idxs)\n",
    "        test_dataloader = DataLoader(test_dataset_idxs, batch_size, shuffle=True)  \n",
    "\n",
    "        #====================== モデルの取得、optimizer の設定 ==============\n",
    "        label_estimator = Model().to(device)\n",
    "\n",
    "\n",
    "        # 事前学習済みの smiles_encoder のパラメタを読み込む\n",
    "        print('fold = ', _fold)\n",
    "        pf_symbol_en = hpara['pretrain_path']+'/'+'OdorCode-40 symbol_encoder D'+str(hpara['DimEmbed'])+'.Hidden'+str(hpara['DimTfHidden'])+'.Head'+ str(hpara['NumHead'])+'.L'+str(hpara['NumLayers'])+'.R'+str(hpara['MaskRate'])+'.S'+str(TotalSize)+'-epoch.'+str(ModelEpoch)\n",
    "        pf_smiles_en = hpara['pretrain_path']+'/'+'OdorCode-40 smiles_encoder D'+str(hpara['DimEmbed'])+'.Hidden'+str(hpara['DimTfHidden'])+'.Head'+ str(hpara['NumHead'])+'.L'+str(hpara['NumLayers'])+'.R'+str(hpara['MaskRate'])+'.S'+str(TotalSize)+'-epoch.'+str(ModelEpoch)\n",
    "        print('reading '+pf_symbol_en)\n",
    "        label_estimator.symbol_encoder.load_state_dict(torch.load(pf_symbol_en))\n",
    "        print('reading '+pf_smiles_en)\n",
    "        label_estimator.smiles_encoder.load_state_dict(torch.load(pf_smiles_en))\n",
    "\n",
    "        for param in label_estimator.symbol_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in label_estimator.smiles_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in label_estimator.smiles_encoder.transformer_encoder.layers[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        optimizer_label_estimator = optim.Adam([{'params': label_estimator.smiles_encoder.transformer_encoder.layers[-1].parameters(), 'lr': hpara['LearningRate']*rate1},\n",
    "                                                    {'params': label_estimator.fnn2.parameters(), 'lr': hpara['LearningRate']},\n",
    "                                                    {'params': label_estimator.fnn.parameters(), 'lr': hpara['LearningRate']}])\n",
    "        \n",
    "        for epoch in range(NumEpoch):\n",
    "            #---------------- train step -----------------\n",
    "            sample_num = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if task_type == 'classification': \n",
    "                total_tps = torch.zeros(lID).to(device)\n",
    "                total_real_pos = torch.zeros(lID).to(device)\n",
    "                total_pred_pos = torch.zeros(lID).to(device)\n",
    "\n",
    "            label_estimator.train()\n",
    "\n",
    "            for idxs in train_dataloader:\n",
    "                # 此处随机调换smiles root\n",
    "                smiles = pad_sequence([torch.tensor([BOS_ID]+change_smiles(canonical_smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                # smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                labels = torch.tensor([labels_vec_list[idx] for idx in idxs],dtype=torch.float).to(device)\n",
    "\n",
    "                optimizer_label_estimator.zero_grad()\n",
    "\n",
    "                estimated_labels = label_estimator(smiles)\n",
    "\n",
    "                loss = criterion_label(estimated_labels, labels)\n",
    "                \n",
    "                loss.backward()     # 誤差逆伝播 \n",
    "\n",
    "                optimizer_label_estimator.step()  # パラメータ更新\n",
    "\n",
    "                total_loss += loss.item()*len(idxs)\n",
    "                sample_num += len(idxs)\n",
    "\n",
    "                if task_type == 'classification': \n",
    "                    total_tps += seek_TP(estimated_labels, labels).sum(0)\n",
    "                    total_real_pos += (positive(labels).int()).sum(0)\n",
    "                    total_pred_pos += (positive(estimated_labels).int()).sum(0)\n",
    "\n",
    "            mean_loss = total_loss / sample_num\n",
    "            list_train_loss.append(mean_loss)\n",
    "\n",
    "            # metrics计算\n",
    "            if task_type == 'classification':\n",
    "                precision = total_tps/(total_pred_pos + 1e-9)\n",
    "                recall    = total_tps/(total_real_pos + 1e-9)\n",
    "                f1 = 2.0*precision*recall/(precision+recall+1e-9) # (lID, )\n",
    "                eachODFold[_fold, epoch] = f1\n",
    "                micro_mean_precision = total_tps.sum()/total_pred_pos.sum()\n",
    "                micro_mean_recall    = total_tps.sum()/total_real_pos.sum()      \n",
    "                metrics = 2*micro_mean_precision*micro_mean_recall/(micro_mean_precision+micro_mean_recall)\n",
    "                metrics = metrics.item()\n",
    "            else:\n",
    "                metrics = math.sqrt(mean_loss)\n",
    "                eachODFold[_fold, epoch, 0] = metrics \n",
    "\n",
    "            list_train_metrics.append(metrics)\n",
    "\n",
    "            avgOD_metrics[_fold, epoch] = metrics # tensorboard记录结果\n",
    "            \n",
    "            # tensorboard画图\n",
    "            writter.add_scalars('fold%s-microF1orRMSE/train' % _fold, {para_name: metrics}, epoch)\n",
    "            if _fold == fold_cv-1:\n",
    "                macro_f1 = torch.sum(eachODFold[:, epoch, :]).item() / (fold_cv*lID)\n",
    "                writter.add_scalars('AvgFold-macroF1orRMSE/train', {para_name: macro_f1}, epoch) \n",
    "                for i in range(lID):\n",
    "                    od_f1 = torch.sum(eachODFold[:, epoch, i]).item() / fold_cv\n",
    "                    writter.add_scalars('AvgFold-%s/train' % ID2label[i], {para_name:od_f1}, epoch)\n",
    "\n",
    "\n",
    "            #--------------- test step ------------------\n",
    "            sample_num = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            if task_type == 'classification':\n",
    "                total_tps = torch.zeros(lID).to(device)\n",
    "                total_real_pos = torch.zeros(lID).to(device)\n",
    "                total_pred_pos = torch.zeros(lID).to(device)\n",
    "\n",
    "            label_estimator.eval()\n",
    "\n",
    "            labels_test = torch.zeros([1, lID]).to(device)\n",
    "            preds_test = torch.zeros([1, lID]).to(device)\n",
    "\n",
    "            for idxs in test_dataloader:\n",
    "                smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "                labels = torch.tensor([labels_vec_list[idx] for idx in idxs],dtype=torch.float).to(device)\n",
    "\n",
    "                estimated_labels = label_estimator(smiles)\n",
    "\n",
    "                loss = criterion_label(estimated_labels, labels)\n",
    "\n",
    "                total_loss += loss.item()*len(idxs)\n",
    "                sample_num += len(idxs)\n",
    "\n",
    "                if task_type == 'classification': \n",
    "                    total_tps += seek_TP(estimated_labels, labels).sum(0)\n",
    "                    total_real_pos += (positive(labels).int()).sum(0)\n",
    "                    total_pred_pos += (positive(estimated_labels).int()).sum(0)\n",
    "\n",
    "                labels_test = torch.concat((labels_test, labels), dim=0)\n",
    "                preds_test = torch.concat((preds_test, estimated_labels), dim=0)\n",
    "\n",
    "            labels_test = labels_test[1:]\n",
    "            preds_test = preds_test[1:]\n",
    "\n",
    "            mean_loss = total_loss / sample_num\n",
    "            list_test_loss.append(mean_loss)\n",
    "\n",
    "            # metrics计算\n",
    "            if task_type == 'classification':\n",
    "                precision = total_tps/(total_pred_pos + 1e-9)\n",
    "                recall    = total_tps/(total_real_pos + 1e-9)\n",
    "                f1 = 2.0*precision*recall/(precision+recall+ 1e-9)\n",
    "                eachODFold_test[_fold, epoch] = f1\n",
    "                micro_mean_precision = total_tps.sum()/total_pred_pos.sum()\n",
    "                micro_mean_recall    = total_tps.sum()/total_real_pos.sum()     \n",
    "                metrics = 2*micro_mean_precision*micro_mean_recall/(micro_mean_precision+micro_mean_recall)\n",
    "                metrics = metrics.item()\n",
    "            else:\n",
    "                metrics = math.sqrt(mean_loss)\n",
    "                eachODFold_test[_fold, epoch, 0] = metrics\n",
    "\n",
    "            list_test_metrics.append(metrics)\n",
    "\n",
    "            avgOD_metrics_test[_fold, epoch] = metrics # tensorboard记录结果\n",
    "\n",
    "            # tensorboard画图\n",
    "            writter.add_scalars('fold%s-microF1orRMSE/test' % _fold, {para_name: metrics}, epoch)\n",
    "            if _fold == fold_cv-1:\n",
    "                macro_f1 = torch.sum(eachODFold_test[:, epoch, :]).item() / (fold_cv*lID)\n",
    "                writter.add_scalars('AvgFold-macroF1orRMSE/test', {para_name: macro_f1}, epoch) \n",
    "                f1_eachod = torch.sum(eachODFold_test[:,epoch,:], dim=0) / fold_cv # shape=(lID, )\n",
    "                for i in range(lID):\n",
    "                    # od_f1 = torch.sum(eachODFold_test[:, epoch, i]).item() / fold_cv\n",
    "                    writter.add_scalars('AvgFold-%s/test' % ID2label[i], {para_name: f1_eachod[i].item()}, epoch)\n",
    "\n",
    "            # 最佳结果记录\n",
    "            if task_type == 'classification':\n",
    "                if macro_f1 > best_metrics[i]:\n",
    "                    best_metrics[i] = macro_f1\n",
    "                    best_logits_pred[i] = preds_test.tolist() # shape=(batch, lID)\n",
    "                    best_labels[i] = labels_test.tolist()\n",
    "            else:\n",
    "                if macro_f1 < best_metrics[i]:\n",
    "                    best_metrics[i] = macro_f1\n",
    "                    best_logits_pred[i] = preds_test.tolist() # shape=(batch, lID)\n",
    "                    best_labels[i] = labels_test.tolist()\n",
    "\n",
    "    # 计算auc, r_square\n",
    "    rocauc_macro = 0\n",
    "    pr_auc_macro = 0\n",
    "    rocauc_micro = 0\n",
    "    pr_auc_micro = 0\n",
    "    r_2 = 0\n",
    "    if task_type == 'classification':\n",
    "        for i in range(fold_cv):\n",
    "            rocauc_macro += roc_auc_score(best_labels[i], best_logits_pred[i], average='macro')\n",
    "            pr_auc_macro += average_precision_score(best_labels[i], best_logits_pred[i], average='macro')\n",
    "            rocauc_micro += roc_auc_score(best_labels[i], best_logits_pred[i], average='micro')\n",
    "            pr_auc_micro += average_precision_score(best_labels[i], best_logits_pred[i], average='micro')\n",
    "    else:\n",
    "        for i in range(fold_cv):\n",
    "            r_2 += r2_score(best_labels[i], best_logits_pred[i])\n",
    "\n",
    "    # tensorboard hpara结果记录\n",
    "    result_dict = {'macroF1 or RMSE': sum(best_metrics)/fold_cv, \n",
    "    'ROCAUC_macro': rocauc_macro/fold_cv, 'ROCAUC_micro': rocauc_micro/fold_cv,\n",
    "    'PRAUC macro': pr_auc_macro/fold_cv, 'PRAUC micro': pr_auc_micro/fold_cv, \n",
    "    'r^2': r_2/fold_cv}\n",
    "\n",
    "    # for i in range(lID):\n",
    "    #     result_dict[ID2label[i]] = best_F1_eachod[i].item()\n",
    "    writter.add_hparams(hpara,\n",
    "                            result_dict, \n",
    "                            run_name=para_name\n",
    "                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conduct experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "hpara = {\n",
    "        'pretrain_path': 'modelsave0203',\n",
    "        'DimEmbed': 256,\n",
    "        'DimTfHidden': 512,\n",
    "        'NumHead': 8,\n",
    "        'NumLayers': 10,\n",
    "        'MaskRate': 0.5,\n",
    "        'LearningRate': 0.0003, # od推测为0.0005 learningrate是od模型的，不是pretrain模型的\n",
    "        'Dropout': 0.1\n",
    "    } \n",
    "\n",
    "# モデルのパラメタ\n",
    "DimEmbed = hpara['DimEmbed']     # 記号の embedding の次元\n",
    "DimTfHidden = hpara['DimTfHidden']  # TransformerEncoder の FNNの中間層の次元\n",
    "NumHead = hpara['NumHead']        # TransformerEncoder の multi-head の数\n",
    "NumLayers = hpara['NumLayers']      # TransformerEncoder の transformer 層の数\n",
    "NormFirst = True   # Vision Transformer に合わせてみた\n",
    "Activation = 'gelu' # Bert や Vision Transformer に合わせてみた\n",
    "\n",
    "MaskRate = hpara['MaskRate']\n",
    "TotalSize = 100000\n",
    "ModelEpoch = 600\n",
    "\n",
    "# 学習のパラメタ\n",
    "LearningRate = hpara['LearningRate']\n",
    "NumEpoch = 200\n",
    "Dropout = hpara['Dropout']\n",
    "batch_size = 32\n",
    "InitRange = 0.1\n",
    "\n",
    "# writter\n",
    "record_dict = 'tensorboard_logs/torch/0307-5fold/ESOL/'\n",
    "writter = SummaryWriter(record_dict)\n",
    "\n",
    "main_program(1.0, hpara)\n",
    "\n",
    "writter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
