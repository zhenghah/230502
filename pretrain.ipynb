{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read symbol dictionary\n",
    "the symbol dictionary is saved as 'OdorCode-40 Symbol Dictionary' by running 'datasetMake_pretrain.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_SMILES_LENGTH = 100\n",
    "\n",
    "f = open('CHEMBL/OdorCode-40 Symbol Dictionary', 'rb') \n",
    "[symbol_ID, ID_symbol, sID] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "PAD_ID = 0\n",
    "CLS_ID = 1\n",
    "BOS_ID = 2\n",
    "EOS_ID = 3\n",
    "MSK_ID = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some functions\n",
    "(1) smiles_str2smiles: translate a SMILES to a list of symbols ID\n",
    "\n",
    "(2) smiles2smiles_str: translate a list of symbols ID to a SMILES\n",
    "\n",
    "(3) masking: mask SMILES that are input to the second encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------#\n",
    "#           smiles_str2smiles      #\n",
    "#----------------------------------#\n",
    "# transpose smiles string to the list of IDs \n",
    "\n",
    "max_length_symbol = max([len(s) for s in ID_symbol])\n",
    "\n",
    "def smiles_str2smiles(smiles_str, flag=False): \n",
    "  \"smiles を記号の列に変換（長さ2のNaなどの元素記号も1つのindexに変換）\"\n",
    "\n",
    "  smiles = []\n",
    "  i=0\n",
    "  while i < len(smiles_str):\n",
    "    NotFindID = True\n",
    "    for j in range(max_length_symbol,0,-1) :\n",
    "      if i+j <= len(smiles_str) and smiles_str[i:i+j] in symbol_ID: \n",
    "        smiles.append(symbol_ID[smiles_str[i:i+j]])\n",
    "        i += j-1 \n",
    "        NotFindID = False\n",
    "        break\n",
    "    if NotFindID:\n",
    "      # print('something wrong on converting smiles_str to smiles')\n",
    "      break\n",
    "    i += 1\n",
    "  return smiles\n",
    "\n",
    "#----------------------------------#\n",
    "#           smiles2smiles_str      #\n",
    "#----------------------------------#\n",
    "def smiles2smiles_str(smiles): \n",
    "  smiles_str = ''\n",
    "  for id in smiles:\n",
    "    smiles_str += ID_symbol[id]\n",
    "  return smiles_str\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "#             masking              #\n",
    "#----------------------------------#\n",
    "MaskRate = 0.1   \n",
    "def masking(smiles):\n",
    "  smiles_tmp = []\n",
    "  for s in smiles:\n",
    "    p = random.random()\n",
    "    if p<MaskRate:\n",
    "      smiles_tmp = smiles_tmp + [MSK_ID]\n",
    "    else:\n",
    "      smiles_tmp = smiles_tmp + [s]\n",
    "  return smiles_tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data for pretraining\n",
    "read pairs of input and target SMILES from file 'CHEMBL/OdorCode-40 Pretrain MLM_data' which is obtained by running 'datasetMake_pretrain.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('CHEMBL/OdorCode-40 Pretrain MLM_data','rb')\n",
    "[canonical_smiles_list, smiles_list] = pickle.load(f) \n",
    "f.close()\n",
    "\n",
    "print('Sample size : ', len(smiles_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "2-encoder model for pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitRange = 0.1\n",
    "NumToken = sID\n",
    "\n",
    "MaskRate = 0.1\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=2048):  # d_model: dimensional of embeddings\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) #.transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "class SymbolEncoder(nn.Module):\n",
    "    def __init__(self, num_token, d_model):  \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(num_token, d_model, padding_idx=PAD_ID)\n",
    "        self.embed.weight.data.uniform_(-InitRange, InitRange)  # embedding init\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embed(src) * math.sqrt(self.d_model)\n",
    "        return src\n",
    "#----------------------------------------------------------------------------\n",
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_head, d_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, num_head, dim_feedforward=d_hidden, norm_first = NormFirst, activation=Activation, dropout=Dropout, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NumLayers, norm=encoder_norm)     \n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask = padding_mask)\n",
    "        return x\n",
    "\n",
    "#--------------ここから--------------------------------------------------------------\n",
    "class MLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop1 = nn.Dropout(p=Dropout)\n",
    "        self.drop2 = nn.Dropout(p=Dropout)\n",
    "        self.positional_encoder = PositionalEncoder(DimEmbed)\n",
    "        self.symbol_encoder = SymbolEncoder(NumToken, DimEmbed) \n",
    "        self.smiles_encoder1 = MyTransformerEncoder(DimEmbed, NumHead, DimTfHidden)\n",
    "        self.smiles_encoder2 = MyTransformerEncoder(DimEmbed, NumHead, DimTfHidden)\n",
    "        self.fnn = nn.Linear(DimEmbed, NumToken)\n",
    "\n",
    "    def forward(self, canonical_smiles, smiles_masked, flag):\n",
    "        # flag is used to inditifiy whether add molecular embedding or not\n",
    "        cls = torch.ones(canonical_smiles.size(0),1,dtype=torch.long).to(device)\n",
    "\n",
    "        # add cls before canonical_smiles、compute padding_mask1 then\n",
    "        # symbol_encoding, add positional_encoding (include dropout) \n",
    "        # molecular embedding is obtained by smiles_encoder1 \n",
    "        if flag:\n",
    "          x1 = torch.concat((cls,canonical_smiles),dim=1)  # add cls \n",
    "          padding_mask1 = (x1 == PAD_ID)\n",
    "          x1 = self.drop1(self.symbol_encoder(x1) + self.positional_encoder(x1)) \n",
    "          x1 = self.smiles_encoder1(x1, padding_mask1)\n",
    "          embed = x1[:,0,:]\n",
    "\n",
    "        # add cls before smiles_masked, compute padding_mask2 then\n",
    "        # symbol_encoding ,\n",
    "        # if flag == True use molecular embedding obtained from smiles_encoder1 to replace the first token in the inputs\n",
    "        # add positional_encoding (include dropout) \n",
    "        # return the outputs of smiles_encoder2 \n",
    "        x2 = torch.concat((cls, smiles_masked),dim=1)  # add cls \n",
    "        padding_mask2 = (x2 == PAD_ID) \n",
    "        x2 = self.symbol_encoder(x2)\n",
    "        if flag:\n",
    "          x2[:,0,:] = embed  # use molecular embedding obtained from smiles_encoder1 to replace the first token in the inputs\n",
    "        x2 = self.drop2(x2 + self.positional_encoder(x2))\n",
    "        y = self.smiles_encoder2(x2, padding_mask2)\n",
    "        y = self.fnn(y[:,1:,:]) \n",
    "        return y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for conducting experiment\n",
    "The first encoder will be saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction = 'mean')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def main_program(flag=True):\n",
    "\n",
    "  global list_train_loss\n",
    "  global list_train_acc\n",
    "  global list_test_loss\n",
    "  global list_test_acc\n",
    "\n",
    "  TrainSize = int(TotalSize*4/5)\n",
    "  TestSize = TotalSize-TrainSize\n",
    "\n",
    "  all_idx_list = list(range(TotalSize))\n",
    "  train_idx_list, test_idx_list = torch.utils.data.random_split(all_idx_list, [TrainSize, TestSize])\n",
    "\n",
    "  train_dataloader = DataLoader(train_idx_list, batch_size, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_idx_list, batch_size, shuffle=True) \n",
    "  \n",
    "  #====================== model init, set optimizer ==============\n",
    "  model = MLM().to(device)\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=LearningRate)\n",
    "\n",
    "  list_train_loss = []\n",
    "  list_train_acc = []\n",
    "  list_test_loss = []\n",
    "  list_test_acc = []\n",
    "\n",
    "  pf_symbol_en = 'OdorCode-40 symbol_encoder D'+str(DimEmbed)+'.Hidden'+str(DimTfHidden)+'.Head'+str(NumHead)+'.L'+str(NumLayers)+'.R'+str(MaskRate)+'.S'+str(TotalSize)\n",
    "  pf_smiles_en = 'OdorCode-40 smiles_encoder D'+str(DimEmbed)+'.Hidden'+str(DimTfHidden)+'.Head'+str(NumHead)+'.L'+str(NumLayers)+'.R'+str(MaskRate)+'.S'+str(TotalSize)\n",
    "\n",
    "  for epoch in range(1,NumEpoch+1):\n",
    "\n",
    "    #---------------- train step -----------------\n",
    "    sample_num = 0\n",
    "    total_loss = 0\n",
    "    num_mask = 0\n",
    "    num_notmp = 0\n",
    "    num_success_mask = 0\n",
    "    num_success_notmp = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idxs in train_dataloader:\n",
    "      canonical_smiles = pad_sequence([torch.tensor([BOS_ID]+canonical_smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      target_smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      masked_smiles = pad_sequence([torch.tensor([BOS_ID]+masking(smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      estimated_smiles_prob = model(canonical_smiles, masked_smiles, flag)\n",
    "\n",
    "      k1 = estimated_smiles_prob.size(0)*estimated_smiles_prob.size(1)\n",
    "      k2 = target_smiles.size(0)*target_smiles.size(1)\n",
    "      if k1 != k2:\n",
    "        print('?????')\n",
    "        exit()\n",
    "      \n",
    "      loss = criterion(estimated_smiles_prob.view(k1,-1), target_smiles.view(k2)) \n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      sample_num += len(idxs)\n",
    "      total_loss += loss.item()*len(idxs)  \n",
    "\n",
    "      # count the number of symbols recovery succcessy \n",
    "      estimated_smiles = torch.argmax(estimated_smiles_prob, dim=2)\n",
    "      equal_element = (target_smiles == estimated_smiles).int()\n",
    "      mask_element = (masked_smiles == MSK_ID)\n",
    "      notmask_element = (masked_smiles != MSK_ID).int()\n",
    "      notpad_element = (masked_smiles != PAD_ID).int()\n",
    "      notmp_element = torch.mul(notmask_element, notpad_element)\n",
    "      num_mask += torch.sum(mask_element)\n",
    "      num_notmp += torch.sum(notmp_element)\n",
    "      num_success_mask += torch.sum(torch.mul(equal_element, mask_element))\n",
    "      num_success_notmp += torch.sum(torch.mul(equal_element, notmp_element))\n",
    "\n",
    "      del canonical_smiles\n",
    "      del masked_smiles\n",
    "      del target_smiles\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    mean_loss = total_loss/sample_num\n",
    "    acc_mask = (100 * num_success_mask / num_mask).item()\n",
    "    acc_notmp  = (100 * num_success_notmp / num_notmp).item()\n",
    "    print('%4d'%epoch, '  %6.4f'%mean_loss,  '  %6.4f'%acc_mask, '  %6.4f'%acc_notmp, end='  ')\n",
    "\n",
    "    list_train_loss.append(mean_loss)\n",
    "    list_train_acc.append(acc_mask)\n",
    "\n",
    "    if epoch % 50 == 0:      \n",
    "      torch.save(model.smiles_encoder1.state_dict(), 'modelsave/'+pf_smiles_en+'-epoch.'+str(epoch))\n",
    "      torch.save(model.symbol_encoder.state_dict(),  'modelsave/'+pf_symbol_en+'-epoch.'+str(epoch))\n",
    "\n",
    "    #--------------- test step ------------------\n",
    "    sample_num = 0\n",
    "    total_loss = 0\n",
    "    num_mask = 0\n",
    "    num_notmp = 0\n",
    "    num_success_mask = 0\n",
    "    num_success_notmp = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idxs in test_dataloader:\n",
    "      canonical_smiles = pad_sequence([torch.tensor([BOS_ID]+canonical_smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      target_smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      masked_smiles = pad_sequence([torch.tensor([BOS_ID]+masking(smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "\n",
    "      estimated_smiles_prob = model(canonical_smiles, masked_smiles, flag) # (batch, len_seq, num_tokens)\n",
    "\n",
    "      k1 = estimated_smiles_prob.size(0)*estimated_smiles_prob.size(1)\n",
    "      k2 = target_smiles.size(0)*target_smiles.size(1)\n",
    "      if k1 != k2:\n",
    "        print('?????')\n",
    "        exit()\n",
    "      \n",
    "      loss = criterion(estimated_smiles_prob.view(k1,-1), target_smiles.view(k2)) \n",
    "\n",
    "      sample_num += len(idxs)\n",
    "      total_loss += loss.item()*len(idxs)  \n",
    "\n",
    "      # count the number of symbols recovery succcessy \n",
    "      estimated_smiles = torch.argmax(estimated_smiles_prob, dim=2)\n",
    "      equal_element = (target_smiles == estimated_smiles).int()\n",
    "      mask_element = (masked_smiles == MSK_ID)\n",
    "      notmask_element = (masked_smiles != MSK_ID).int()\n",
    "      notpad_element = (masked_smiles != PAD_ID).int()\n",
    "      notmp_element = torch.mul(notmask_element, notpad_element) \n",
    "      num_mask += torch.sum(mask_element)\n",
    "      num_notmp += torch.sum(notmp_element)\n",
    "      num_success_mask += torch.sum(torch.mul(equal_element, mask_element))\n",
    "      num_success_notmp += torch.sum(torch.mul(equal_element, notmp_element))\n",
    "\n",
    "      del canonical_smiles\n",
    "      del masked_smiles\n",
    "      del target_smiles\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    mean_loss = total_loss/sample_num\n",
    "    acc_mask = (100 * num_success_mask / num_mask).item()\n",
    "    acc_notmp  = (100 * num_success_notmp / num_notmp).item()\n",
    "    print('%4d'%epoch, '  %6.4f'%mean_loss,  '  %6.4f'%acc_mask, '  %6.4f'%acc_notmp)\n",
    "\n",
    "    '''\n",
    "    print(buf0)\n",
    "    print(buf1)\n",
    "    print(buf2)\n",
    "    print(buf3)\n",
    "    print('')\n",
    "    print(smiles2smiles_str(canonical_smiles[0]))\n",
    "    print(smiles2smiles_str(target_smiles[0]))\n",
    "    print(smiles2smiles_str(masked_smiles[0]))\n",
    "    print(vec2string(estimated_smiles_prob[0]))\n",
    "    '''\n",
    "\n",
    "    list_test_loss.append(mean_loss)\n",
    "    list_test_acc.append(acc_mask)\n",
    "\n",
    "\n",
    "  # save model's parameters \n",
    "  torch.save(model.smiles_encoder1.state_dict(), pf_smiles_en+'-epoch.'+str(NumEpoch))\n",
    "  torch.save(model.symbol_encoder.state_dict(),  pf_symbol_en+'-epoch.'+str(NumEpoch))\n",
    "\n",
    "  %matplotlib inline\n",
    "  Figure = plt.figure(figsize=(10,10))\n",
    "  ax1 = Figure.add_subplot(2,2,1)\n",
    "  ax2 = Figure.add_subplot(2,2,2)\n",
    "  ax3 = Figure.add_subplot(2,2,3)\n",
    "  ax4 = Figure.add_subplot(2,2,4)\n",
    "\n",
    "  ax1.plot(list_train_loss)\n",
    "  ax2.plot(list_train_acc)\n",
    "  ax3.plot(list_test_loss)\n",
    "  ax4.plot(list_test_acc)\n",
    "\n",
    "  del model\n",
    "  del loss\n",
    "\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conduct experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# hyperparameter setting\n",
    "DimEmbed = 256     # dimensional of embedding \n",
    "DimTfHidden = 256  # dimensional of FNN in TransformerEncoder \n",
    "NumHead = 16       # the number of multi-head in TransformerEncoder \n",
    "NumLayers = 10     # the number of layers in \n",
    "NormFirst = True   \n",
    "Activation = 'gelu' \n",
    "MaskRate = 0.5\n",
    "\n",
    "# 学習のパラメタ\n",
    "LearningRate = 0.0005 \n",
    "NumEpoch = 800\n",
    "Dropout = 0.1 \n",
    "batch_size = 128\n",
    "InitRange = 0.1\n",
    "\n",
    "TotalSize = 100000\n",
    "\n",
    "main_program(True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
