{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read symbol dictionary\n",
    "the symbol dictionary is saved as 'OdorCode-40 Symbol Dictionary' by running 'datasetMake_pretrain.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_SMILES_LENGTH = 100\n",
    "\n",
    "f = open('CHEMBL/OdorCode-40 Symbol Dictionary', 'rb') \n",
    "[symbol_ID, ID_symbol, sID] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "PAD_ID = 0\n",
    "CLS_ID = 1\n",
    "BOS_ID = 2\n",
    "EOS_ID = 3\n",
    "MSK_ID = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some functions\n",
    "(1) smiles_str2smiles: translate a SMILES to a list of symbols ID\n",
    "\n",
    "(2) smiles2smiles_str: translate a list of symbols ID to a SMILES\n",
    "\n",
    "(3) masking: mask SMILES that are input to the second encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------#\n",
    "#           smiles_str2smiles      #\n",
    "#----------------------------------#\n",
    "# smiles を記号（1文字記号だけでなく，2文字，3文字記号も含む）のＩＤのリストに変換\n",
    "\n",
    "max_length_symbol = max([len(s) for s in ID_symbol])\n",
    "\n",
    "def smiles_str2smiles(smiles_str, flag=False): \n",
    "  \"smiles を記号の列に変換（長さ2のNaなどの元素記号も1つのindexに変換）\"\n",
    "\n",
    "  smiles = []\n",
    "  i=0\n",
    "  while i < len(smiles_str):\n",
    "    NotFindID = True\n",
    "    for j in range(max_length_symbol,0,-1) :\n",
    "      if i+j <= len(smiles_str) and smiles_str[i:i+j] in symbol_ID: # j長さの文字記号として辞書登録済み\n",
    "        smiles.append(symbol_ID[smiles_str[i:i+j]])\n",
    "        i += j-1 # while ブロックの最後で i++ されるが，j文字なので余分に i+=j-1\n",
    "        NotFindID = False\n",
    "        break\n",
    "    if NotFindID:\n",
    "      # print('something wrong on converting smiles_str to smiles')\n",
    "      break\n",
    "    i += 1\n",
    "  return smiles\n",
    "\n",
    "#----------------------------------#\n",
    "#           smiles2smiles_str      #\n",
    "#----------------------------------#\n",
    "def smiles2smiles_str(smiles): \n",
    "  smiles_str = ''\n",
    "  for id in smiles:\n",
    "    smiles_str += ID_symbol[id]\n",
    "  return smiles_str\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "#             masking              #\n",
    "#----------------------------------#\n",
    "MaskRate = 0.1   # とりあえず0.1で定義。実験時には再設定可\n",
    "def masking(smiles):\n",
    "  smiles_tmp = []\n",
    "  for s in smiles:\n",
    "    p = random.random()\n",
    "    if p<MaskRate:\n",
    "      smiles_tmp = smiles_tmp + [MSK_ID]\n",
    "    else:\n",
    "      smiles_tmp = smiles_tmp + [s]\n",
    "  return smiles_tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data for pretraining\n",
    "read pairs of input and target SMILES from file 'CHEMBL/OdorCode-40 Pretrain MLM_data' which is obtained by running 'datasetMake_pretrain.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('CHEMBL/OdorCode-40 Pretrain MLM_data','rb')\n",
    "[canonical_smiles_list, smiles_list] = pickle.load(f) \n",
    "f.close()\n",
    "\n",
    "print('Sample size : ', len(smiles_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "2-encoder model for pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitRange = 0.1\n",
    "NumToken = sID\n",
    "\n",
    "MaskRate = 0.1\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=2048):  # d_model: 1記号のembeddingの次元, max_len : 最大のbatchサイズ？\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) #.transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "class SymbolEncoder(nn.Module):\n",
    "    def __init__(self, num_token, d_model):  \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(num_token, d_model, padding_idx=PAD_ID)\n",
    "        self.embed.weight.data.uniform_(-InitRange, InitRange)  # embedding の再初期化\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embed(src) * math.sqrt(self.d_model)\n",
    "        return src\n",
    "#----------------------------------------------------------------------------\n",
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_head, d_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, num_head, dim_feedforward=d_hidden, norm_first = NormFirst, activation=Activation, dropout=Dropout, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, NumLayers, norm=encoder_norm)     \n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask = padding_mask)\n",
    "        return x\n",
    "\n",
    "#--------------ここから--------------------------------------------------------------\n",
    "class MLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop1 = nn.Dropout(p=Dropout)\n",
    "        self.drop2 = nn.Dropout(p=Dropout)\n",
    "        self.positional_encoder = PositionalEncoder(DimEmbed)\n",
    "        self.symbol_encoder = SymbolEncoder(NumToken, DimEmbed) # ＊＊＊＊　symbol_encoder が１つで良いのか検討？　＊＊＊＊＊\n",
    "        self.smiles_encoder1 = MyTransformerEncoder(DimEmbed, NumHead, DimTfHidden)\n",
    "        self.smiles_encoder2 = MyTransformerEncoder(DimEmbed, NumHead, DimTfHidden)\n",
    "        self.fnn = nn.Linear(DimEmbed, NumToken)\n",
    "\n",
    "    def forward(self, canonical_smiles, smiles_masked, flag):\n",
    "        # flag == False の場合は、canonical_smiles の embed を先頭に追加しない（clsのまま）\n",
    "        cls = torch.ones(canonical_smiles.size(0),1,dtype=torch.long).to(device)\n",
    "\n",
    "        # canonical_smiles の先頭に cls を挿入し、padding_mask1 を求めた後、\n",
    "        # symbol_encoding を施し、positional_encoding を追加（ここで Dropout を適用） \n",
    "        # smiles_encoder1 で canonical_smiles の embedding（= embed）を求める\n",
    "        if flag:\n",
    "          x1 = torch.concat((cls,canonical_smiles),dim=1)  # 先頭に cls ('_', ID 1) を挿入\n",
    "          padding_mask1 = (x1 == PAD_ID)\n",
    "          x1 = self.drop1(self.symbol_encoder(x1) + self.positional_encoder(x1)) \n",
    "          x1 = self.smiles_encoder1(x1, padding_mask1)\n",
    "          embed = x1[:,0,:]\n",
    "\n",
    "        # smiles_masked の先頭に cls を挿入し、padding_mask2 を求めた後、\n",
    "        # symbol_encoding を施し、\n",
    "        # flag == True ならば先頭を smiles_encoder1 で求めた embed に置換し、\n",
    "        # positional_encoding を追加（ここで Dropout を適用）\n",
    "        # これを smiles_encoder2 に入力し出力（予測値）を返す\n",
    "        x2 = torch.concat((cls, smiles_masked),dim=1)  # 先頭に cls ('_', ID 1) を挿入\n",
    "        padding_mask2 = (x2 == PAD_ID) \n",
    "        x2 = self.symbol_encoder(x2)\n",
    "        if flag:\n",
    "          x2[:,0,:] = embed  # 先頭（clsのembedding）を embed に置換\n",
    "        x2 = self.drop2(x2 + self.positional_encoder(x2))\n",
    "        y = self.smiles_encoder2(x2, padding_mask2)\n",
    "        y = self.fnn(y[:,1:,:]) \n",
    "        return y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for conducting experiment\n",
    "The first encoder will be saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction = 'mean')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def main_program(flag=True):\n",
    "\n",
    "  global list_train_loss\n",
    "  global list_train_acc\n",
    "  global list_test_loss\n",
    "  global list_test_acc\n",
    "\n",
    "  TrainSize = int(TotalSize*4/5)\n",
    "  TestSize = TotalSize-TrainSize\n",
    "\n",
    "  all_idx_list = list(range(TotalSize))\n",
    "  train_idx_list, test_idx_list = torch.utils.data.random_split(all_idx_list, [TrainSize, TestSize])\n",
    "\n",
    "  train_dataloader = DataLoader(train_idx_list, batch_size, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_idx_list, batch_size, shuffle=True) # test の最後の mini batch の先頭のデータに対して，smiles を復元したものを表示するので，各 epoch で表示される smiles が変わるように shuffle\n",
    "\n",
    "  #====================== モデルの取得、optimizer の設定 ==============\n",
    "  model = MLM().to(device)\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=LearningRate)\n",
    "\n",
    "  list_train_loss = []\n",
    "  list_train_acc = []\n",
    "  list_test_loss = []\n",
    "  list_test_acc = []\n",
    "\n",
    "  pf_symbol_en = 'OdorCode-40 symbol_encoder D'+str(DimEmbed)+'.Hidden'+str(DimTfHidden)+'.Head'+str(NumHead)+'.L'+str(NumLayers)+'.R'+str(MaskRate)+'.S'+str(TotalSize)\n",
    "  pf_smiles_en = 'OdorCode-40 smiles_encoder D'+str(DimEmbed)+'.Hidden'+str(DimTfHidden)+'.Head'+str(NumHead)+'.L'+str(NumLayers)+'.R'+str(MaskRate)+'.S'+str(TotalSize)\n",
    "\n",
    "  for epoch in range(1,NumEpoch+1):\n",
    "\n",
    "    #---------------- train step -----------------\n",
    "    sample_num = 0\n",
    "    total_loss = 0\n",
    "    num_mask = 0\n",
    "    num_notmp = 0\n",
    "    num_success_mask = 0\n",
    "    num_success_notmp = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idxs in train_dataloader:\n",
    "      canonical_smiles = pad_sequence([torch.tensor([BOS_ID]+canonical_smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      target_smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      masked_smiles = pad_sequence([torch.tensor([BOS_ID]+masking(smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      estimated_smiles_prob = model(canonical_smiles, masked_smiles, flag)\n",
    "\n",
    "      k1 = estimated_smiles_prob.size(0)*estimated_smiles_prob.size(1)\n",
    "      k2 = target_smiles.size(0)*target_smiles.size(1)\n",
    "      if k1 != k2:\n",
    "        print('?????')\n",
    "        exit()\n",
    "      \n",
    "      loss = criterion(estimated_smiles_prob.view(k1,-1), target_smiles.view(k2)) # 没有mask的symbol也会计算loss\n",
    "\n",
    "      # 誤差逆伝播\n",
    "      loss.backward()\n",
    "\n",
    "      # パラメータ更新 \n",
    "      optimizer.step()\n",
    "\n",
    "      sample_num += len(idxs)\n",
    "      total_loss += loss.item()*len(idxs)  # criterion がミニバッチでの平均 loss なので total_loss を求める際にはミニバッチのサイズを掛けて加える\n",
    "\n",
    "      # 復元の成功数のカウント\n",
    "      estimated_smiles = torch.argmax(estimated_smiles_prob, dim=2)\n",
    "      equal_element = (target_smiles == estimated_smiles).int()\n",
    "      mask_element = (masked_smiles == MSK_ID)\n",
    "      notmask_element = (masked_smiles != MSK_ID).int()\n",
    "      notpad_element = (masked_smiles != PAD_ID).int()\n",
    "      notmp_element = torch.mul(notmask_element, notpad_element)\n",
    "      num_mask += torch.sum(mask_element)\n",
    "      num_notmp += torch.sum(notmp_element)\n",
    "      num_success_mask += torch.sum(torch.mul(equal_element, mask_element))\n",
    "      num_success_notmp += torch.sum(torch.mul(equal_element, notmp_element))\n",
    "\n",
    "      del canonical_smiles\n",
    "      del masked_smiles\n",
    "      del target_smiles\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    mean_loss = total_loss/sample_num\n",
    "    acc_mask = (100 * num_success_mask / num_mask).item()\n",
    "    acc_notmp  = (100 * num_success_notmp / num_notmp).item()\n",
    "    print('%4d'%epoch, '  %6.4f'%mean_loss,  '  %6.4f'%acc_mask, '  %6.4f'%acc_notmp, end='  ')\n",
    "\n",
    "    list_train_loss.append(mean_loss)\n",
    "    list_train_acc.append(acc_mask)\n",
    "\n",
    "    if epoch % 50 == 0:      \n",
    "      torch.save(model.smiles_encoder1.state_dict(), 'modelsave/'+pf_smiles_en+'-epoch.'+str(epoch))\n",
    "      torch.save(model.symbol_encoder.state_dict(),  'modelsave/'+pf_symbol_en+'-epoch.'+str(epoch))\n",
    "\n",
    "    #--------------- test step ------------------\n",
    "    sample_num = 0\n",
    "    total_loss = 0\n",
    "    num_mask = 0\n",
    "    num_notmp = 0\n",
    "    num_success_mask = 0\n",
    "    num_success_notmp = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idxs in test_dataloader:\n",
    "      canonical_smiles = pad_sequence([torch.tensor([BOS_ID]+canonical_smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      target_smiles = pad_sequence([torch.tensor([BOS_ID]+smiles_list[idx]+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "      masked_smiles = pad_sequence([torch.tensor([BOS_ID]+masking(smiles_list[idx])+[EOS_ID]) for idx in idxs], batch_first=True).to(device)\n",
    "\n",
    "      estimated_smiles_prob = model(canonical_smiles, masked_smiles, flag) # (batch, len_seq, num_tokens)\n",
    "\n",
    "      k1 = estimated_smiles_prob.size(0)*estimated_smiles_prob.size(1)\n",
    "      k2 = target_smiles.size(0)*target_smiles.size(1)\n",
    "      if k1 != k2:\n",
    "        print('?????')\n",
    "        exit()\n",
    "      \n",
    "      loss = criterion(estimated_smiles_prob.view(k1,-1), target_smiles.view(k2)) # loss计算的是全部SMILES，而不仅是mask的symbol\n",
    "\n",
    "      sample_num += len(idxs)\n",
    "      total_loss += loss.item()*len(idxs)  # criterion がミニバッチでの平均 loss なので total_loss を求める際にはミニバッチのサイズを掛けて加える\n",
    "\n",
    "      # 復元の成功数のカウント\n",
    "      estimated_smiles = torch.argmax(estimated_smiles_prob, dim=2)\n",
    "      equal_element = (target_smiles == estimated_smiles).int()\n",
    "      mask_element = (masked_smiles == MSK_ID)\n",
    "      notmask_element = (masked_smiles != MSK_ID).int()\n",
    "      notpad_element = (masked_smiles != PAD_ID).int()\n",
    "      notmp_element = torch.mul(notmask_element, notpad_element) \n",
    "      num_mask += torch.sum(mask_element)\n",
    "      num_notmp += torch.sum(notmp_element)\n",
    "      num_success_mask += torch.sum(torch.mul(equal_element, mask_element))\n",
    "      num_success_notmp += torch.sum(torch.mul(equal_element, notmp_element))\n",
    "\n",
    "      del canonical_smiles\n",
    "      del masked_smiles\n",
    "      del target_smiles\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    mean_loss = total_loss/sample_num\n",
    "    acc_mask = (100 * num_success_mask / num_mask).item()\n",
    "    acc_notmp  = (100 * num_success_notmp / num_notmp).item()\n",
    "    print('%4d'%epoch, '  %6.4f'%mean_loss,  '  %6.4f'%acc_mask, '  %6.4f'%acc_notmp)\n",
    "\n",
    "    '''\n",
    "    print(buf0)\n",
    "    print(buf1)\n",
    "    print(buf2)\n",
    "    print(buf3)\n",
    "    print('')\n",
    "    print(smiles2smiles_str(canonical_smiles[0]))\n",
    "    print(smiles2smiles_str(target_smiles[0]))\n",
    "    print(smiles2smiles_str(masked_smiles[0]))\n",
    "    print(vec2string(estimated_smiles_prob[0]))\n",
    "    '''\n",
    "\n",
    "    list_test_loss.append(mean_loss)\n",
    "    list_test_acc.append(acc_mask)\n",
    "\n",
    "\n",
    "  # 必要なモデルのパラメタの保存 # 似乎保存了2次最后epoch的模型\n",
    "  torch.save(model.smiles_encoder1.state_dict(), pf_smiles_en+'-epoch.'+str(NumEpoch))\n",
    "  torch.save(model.symbol_encoder.state_dict(),  pf_symbol_en+'-epoch.'+str(NumEpoch))\n",
    "\n",
    "  %matplotlib inline\n",
    "  Figure = plt.figure(figsize=(10,10))\n",
    "  ax1 = Figure.add_subplot(2,2,1)\n",
    "  ax2 = Figure.add_subplot(2,2,2)\n",
    "  ax3 = Figure.add_subplot(2,2,3)\n",
    "  ax4 = Figure.add_subplot(2,2,4)\n",
    "\n",
    "  ax1.plot(list_train_loss)\n",
    "  ax2.plot(list_train_acc)\n",
    "  ax3.plot(list_test_loss)\n",
    "  ax4.plot(list_test_acc)\n",
    "\n",
    "  del model\n",
    "  del loss\n",
    "\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conduct experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# モデルのパラメタ\n",
    "DimEmbed = 256     # 記号の embedding の次元\n",
    "DimTfHidden = 256  # TransformerEncoder の FNNの中間層の次元\n",
    "NumHead = 16       # TransformerEncoder の multi-head の数\n",
    "NumLayers = 10     # TransformerEncoder の transformer 層の数\n",
    "NormFirst = True   # Vision Transformer に合わせてみた\n",
    "Activation = 'gelu' # Bert や Vision Transformer に合わせてみた\n",
    "MaskRate = 0.5\n",
    "\n",
    "# 学習のパラメタ\n",
    "LearningRate = 0.0005 \n",
    "NumEpoch = 800\n",
    "Dropout = 0.1 \n",
    "batch_size = 128\n",
    "InitRange = 0.1\n",
    "\n",
    "TotalSize = 100000\n",
    "\n",
    "main_program(True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
